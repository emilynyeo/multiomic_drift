---
title: "9.process_micom_data"
author: "Emily Yeo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(knitr, data.table, dplyr, tidyr, tableone, kableExtra, readxl,
               readr, car, RColorBrewer, gridExtra, mlbench, earth, ggplot2, 
               AppliedPredictiveModeling, caret, reshape2, corrplot, stringr,
               summarytools, grid, mice, plyr, mlmRev, cowplot, compositions,
               jtools, broom, patchwork, phyloseq, microbiome, glmnet, ISLR,
               MicrobiomeStat, ANCOMBC, ape, vegan, zCompositions, janitor)
```

# Load Micom data

```{r}
micom <- load("/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/micom/WD_Cplex/products_all.RData")
```

# Fliter those on a western diet

```{r}
western <- products.all %>%
  dplyr::filter(diet == "Western") %>%
  dplyr::filter(timepoint2 %in% c("BL", "6m", "12m")) %>% 
  dplyr::select(-c(timepoint2))
```

# Look at flux summary

```{r}
# Summary statistics for flux across sample_id
flux_summary <- western %>%
  dplyr::group_by(sample_id) %>%
  dplyr::summarise(
    mean_flux = mean(flux, na.rm = TRUE),
    sd_flux = sd(flux, na.rm = TRUE),
    min_flux = min(flux, na.rm = TRUE),
    max_flux = max(flux, na.rm = TRUE),
    n = n()  # Count of entries
  )

# View the summary
print(flux_summary)
```

# change structure 

```{r}
# Reshape the western dataframe
flux <- western %>%
  dplyr::select(sample_id, description, flux) %>%  
  pivot_wider(names_from = description, 
              values_from = flux, 
              values_fill = list(flux = 0))

# Convert sample_id to row names
# rownames(path_BL) <- path_BL$sample_id
# path_BL <- path_BL %>% select(-sample_id)  # Remove sample_id column
```

# look at variance 

nearZeroVar diagnoses predictors that have one unique value (i.e. are zero variance predictors) or predictors that are have both of the following characteristics: they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large.

```{r}
nzv_BL <- nearZeroVar(flux,
  freqCut = 95/5,
  uniqueCut = 10,
  saveMetrics = FALSE,
  names = FALSE,
  foreach = FALSE,
  allowParallel = TRUE)

flux_no_nzv <- flux[, -nzv_BL] # only 12 removed 
```

12 with low variance: 

"R total 2 position"                                   
"Glutarate"                                            
"2-Succinyl-6-hydroxy-2,4-cyclohexadiene-1-carboxylate"
"O-Succinylbenzoyl-CoA"                                
"glycolaldehyde"                                       
"glycolate"                                            
"2-Dehydro-3-deoxy-D-gluconate"                        
"D-gluconate"                                          
"(4-hydroxyphenyl)acetaldehyde"                        
"2,3-dihydroxicinnamic acid"                           
"D-glucose"                                            
"Menaquinone 7" 

# Remove variables with low presence thresholds {.tabset}

```{r}
# Set the threshold for percentage of zeros
threshold <- 0.20

# Calculate the percentage of zeros for each column
zero_percentage <- colSums(flux_no_nzv == 0, na.rm = TRUE) / nrow(flux_no_nzv)

# Keep only columns with less than 20% zeros
flux_cleaned <- flux_no_nzv[, zero_percentage < threshold]

# View the cleaned dataframe
dim(flux_cleaned) # 17 removed 
```

# Center and scale

```{r}
# CENTER AND SCALE
# Preprocess the data - imputation and scaling 
preProcValues <- preProcess(flux_cleaned[,2:92], 
                            method = c("scale",
                                       # "center",
                                       "nzv"),
          thresh = 0.95, # cutoff for cumulative % of variance to be retained by PCA
          pcaComp = NULL, #no. PCA components to keep. If specified, over-rides thresh
          na.remove = TRUE, # should missing values be removed from the calculations
          k = 5, # number of nearest neighbors from training set to use for imputation
          knnSummary = mean, # function to average neighbor values/column during imputation
          #outcome = "outcome_BMI_fnl_BL",
          fudge = 0.2, # a tolerance value: Box-Cox transformation lambda values
          numUnique = 15, # no. unique values y has to estimate Box-Cox transformation
          verbose = TRUE, # a logical: prints a log as the computations proceed
          freqCut = 95/5, # cutoff for ratio of most to 2nd most common value.
          uniqueCut = 10, # cutoff % of distinct values out of no. total samples
          cutoff = 0.85, # a numeric value for the pair-wise absolute correlation cutoff.
          rangeBounds = c(0, 1))

flux_all_time_cs <- predict(preProcValues, flux_cleaned[,1:92])
dim(flux_all_time_cs)
```


# plot relative abundances of metabolites 

```{r}
# Reshape the data to long format
long_flux <- flux_all_time_cs %>%
  pivot_longer(cols = -sample_id, 
               names_to = "flux", 
               values_to = "flux_counts")
colnames(long_flux)

# Step 1: Identify the top 10 pathways for each SampleID
top_flux_each_sample <- long_flux %>%
  #dplyr::group_by(SampleID, Pathway) %>%
  #summarize(Total_Abundance = sum(Relative_Abundance, na.rm = TRUE), .groups = "drop") %>%
  group_by(sample_id) %>%
  top_n(10, flux_counts) %>%
  ungroup()

# Step 2: Calculate proportions for the top pathways
proportional_flux <- top_flux_each_sample %>%
  group_by(sample_id) %>%
  mutate(Proportion = flux_counts / sum(flux_counts)) %>%
  ungroup()

# Step 3: Create the proportional bar plot
# Select a subset of samples (e.g., first 15)
subset_data_100 <- proportional_flux %>%
  filter(sample_id %in% unique(sample_id)[1:100]) 

subset_data_200 <- proportional_flux %>%
  filter(sample_id %in% unique(sample_id)[101:200])

subset_data_200_372 <- proportional_flux %>%
  filter(sample_id %in% unique(sample_id)[201:372])

p_200_300 <- ggplot(subset_data_200_372, 
                aes(x = sample_id, y = Proportion, fill = flux)) +
  geom_bar(stat = "identity", position = "fill") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Proportion of Top 10 flux",
       x = "Sample ID", y = "Proportion of Relative Abundance")

# Extract the legend
legend <- get_legend(p_200_300 + theme(legend.position = "right"))
plot_200_300 <- p_200_300 + theme(legend.position = "none")
legend
plot_200_300
```

# make baseline only 

```{r}
sample_names <- flux_all_time_cs$sample_id
baseline_samples <- flux_all_time_cs$sample_id[grep("\\.BL$", sample_names)]

clr_flux_BL <- flux_all_time_cs %>% 
           dplyr::filter(sample_id %in% baseline_samples)
```

# Split into training and testing 

```{r}
# Testing and training meta data 
train_Transformed  <- fread("/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/clinical/transformed/aim1/train_samples_standard_clinical.csv")

test_Transformed  <- fread("/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/clinical/transformed/aim1/test_samples_standard_clinical.csv")
```

# split into aim1 testing and training 

```{r}
# Function to process the names BL
process_names_bl <- function(names) {
  bl_names <- grep("\\.BL$", names, value = TRUE) # ending with "BL"
  extracted_numbers_BL <- sub(".*-(\\d+)\\.BL$", 
                           "\\1", bl_names) # Numbers after "-", before "."
  cleaned_numbers_bl <- sub("^0+", "", extracted_numbers_BL) # Remove leading zeros
  cleaned_numbers_bl} # Return the cleaned numbers

# Function to process the names All
process_names_all <- function(names) {
  all_names <- grep("\\..*$", names, value = TRUE) # all samples 
  extracted_numbers_all <- sub(".*-(\\d+)\\..*$", 
                           "\\1", all_names) 
  cleaned_numbers_all <- sub("^0+", "", extracted_numbers_all) # Remove leading zeros
  cleaned_numbers_all} # Return the cleaned numbers

# Function to process the names, ensuring no mismatches
process_names_all <- function(names) {
  cleaned_numbers_all <- character(length(names)) #vector to store cleaned numbers
  all_names <- grep("\\..*$", names, value = TRUE) #names matching the pattern
  for (i in seq_along(names)) { # Extract numbers and remove leading zeros
    if (names[i] %in% all_names) {
      extracted_number <- sub(".*-(\\d+)\\..*$", "\\1", names[i])
      cleaned_number <- sub("^0+", "", extracted_number)
      cleaned_numbers_all[i] <- cleaned_number
    } else {
      # If the name does not match, keep the original
      cleaned_numbers_all[i] <- names[i]
    }
  }
  return(cleaned_numbers_all)  # Return the cleaned numbers
}

# Apply the function to the sample names
clr_flux_BL$bl_samples <- process_names_bl(clr_flux_BL$sample_id)
flux_all_time_cs$all_samples <- process_names_all(flux_all_time_cs$sample_id)

# Make training & testing to match the samples in training and testing meta
training_sample_names <- train_Transformed$record_id
testing_sample_names <- test_Transformed$record_id

# AIM1 data 
# Step 1: Make column names unique
colnames(clr_flux_BL) <- make.names(colnames(clr_flux_BL), unique = TRUE)
# Step 2: Filter the AIM1 data
BL_flux_testing <- clr_flux_BL %>% filter(bl_samples %in% testing_sample_names)
BL_flux_training <- clr_flux_BL %>% filter(bl_samples %in% training_sample_names)

# AIM2 data
# Step 1: Make column names unique
colnames(flux_all_time_cs) <- make.names(colnames(flux_all_time_cs), unique = TRUE)
# Step 2: Filter the data
all_flux_training <- flux_all_time_cs %>% filter(all_samples %in% training_sample_names)
all_flux_testing <- flux_all_time_cs %>% filter(all_samples %in% testing_sample_names)
```

# Write out files 

```{r}
## aim 1 files
write.csv(BL_flux_training, "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/micom/aim1/flux_BL_training_feb20.csv")
write.csv(BL_flux_testing, "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/micom/aim1/flux_BL_testing_feb20.csv")
write.csv(clr_flux_BL, "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/micom/aim1/flux_BLa_feb20.csv")
# 
# ## aim 2 files
write.csv(all_flux_training, "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/micom/aim2/flux_all_training_feb20.csv")
write.csv(all_flux_testing, "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/micom/aim2/flux_all_testing_feb20.csv")
write.csv(flux_all_time_cs, "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/micom/aim2/flux_all_feb20.csv")
```



