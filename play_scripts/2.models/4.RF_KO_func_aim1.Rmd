---
title: "4.RF_KO_func_aim1.2"
author: "Emily Yeo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(knitr, data.table, dplyr, tidyr, tableone, kableExtra, readxl,
               readr, car, RColorBrewer, gridExtra, mlbench, earth, ggplot2, 
               AppliedPredictiveModeling, caret, reshape2, corrplot, stringr,
               summarytools, grid, mice, plyr, mlmRev, cowplot, compositions,
               jtools, broom, patchwork, phyloseq, microbiome, glmnet, ISLR,
               MicrobiomeStat, ANCOMBC, ape, vegan, zCompositions, janitor,
               treeshap, Metrics, randomForest, xgboost, Ckmeans.1d.dp, caret,
               SHAPforxgboost, doParallel, parallel, tibble, Boruta)
```

# Read in Data

```{r}
# KO pahthway abundance file
KO_dir <- "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/functional/aim1"
path_test <- read.csv(paste(KO_dir, "path_bl_clr_testing.csv", sep = "/")) %>% 
             dplyr::select(-c(X, SampleID))
path_train <- read.csv(paste(KO_dir, "path_bl_clr_training.csv", sep = "/")) %>% 
              dplyr::select(-c(X, SampleID))

# meta
m1_dir <- "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/clinical/transformed/aim1"
test <- read.csv(paste(m1_dir, "test_samples_standard_clinical.csv", sep = "/"))
train <- read.csv(paste(m1_dir, "train_samples_standard_clinical.csv", sep = "/"))

# Merge 
test <- test %>% dplyr::select(c(record_id, outcome_BMI_fnl_BL))
train <- train %>% dplyr::select(c(record_id, outcome_BMI_fnl_BL))

train_path <- merge(path_train, 
                   train, 
                   by.x = "bl_samples", 
                   by.y = "record_id")

test_path <- merge(path_test, 
                  test, 
                   by.x = "bl_samples", 
                  by.y = "record_id")

#rm(path_test, path_train, test, train)
```

# RF with Caret

```{r}
dataset <- train_path
x <- dataset[,2:327]
y <- dataset[,328]

# Basic parameters
seed <- 7
metric <- "RMSE" # Set the performance metric 
set.seed(seed)
mtry <- sqrt(ncol(x)) # Determine the number of variables to try at each split (mtry)
tunegrid <- expand.grid(.mtry=mtry) # Create a tuning grid with the determined mtry value

# Create models with parameters doing a repeated cross-validation method with 10 folds, repeated 10 times.
control_default <- trainControl(method="repeatedcv", number=10, repeats=10)
control_grid <- trainControl(method="repeatedcv", number=10, repeats=10, search="grid")
control_random <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
```

# RF four ways {.tabset}

### RF basic

```{r}
# Perform hyperparameter tuning to find the optimal number of variables to randomly sample at each split (mtry)
mtry <- tuneRF(dataset[2:328],
               dataset$outcome_BMI_fnl_BL, 
               ntreeTry=500,
               stepFactor=1.5,
               improve=0.01, trace=TRUE, plot=TRUE)

# Identify the best mtry value based on the minimum error rate
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]

set.seed(71)
rf <-randomForest(outcome_BMI_fnl_BL~.,data=dataset[2:328], 
                  mtry=best.m, 
                  importance=TRUE,
                  ntree=500)
print(rf)
```

### RF Default

```{r}
# RF Default
rf_default <- train(outcome_BMI_fnl_BL ~., data=dataset[2:328], 
                    method="rf", 
                    metric=metric, 
                    tuneGrid=tunegrid, 
                    trControl=control_default)

print(rf_default)
```

### RF grid search
```{r}
set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:15)) # Create tuning grid for mtry, from 1 to 15
rf_gridsearch <- train(outcome_BMI_fnl_BL~., data=dataset[2:328], 
                       method="rf", 
                       metric=metric, 
                       tuneGrid=tunegrid, 
                       trControl=control_grid)
print(rf_gridsearch)
```

### RF Random
This one takes the longest to run.
```{r}
rf_random <- train(outcome_BMI_fnl_BL~., data=dataset[2:328], 
                   method="rf", 
                   metric=metric, 
                   tuneLength=15, # number of mtry values to try
                   trControl=control_random)
print(rf_random)
```

### RF Manual 
Setting parameters based on RFE suggestions at the bottom 
```{r}
rf <-randomForest(outcome_BMI_fnl_BL~.,
                  data=dataset[2:328], 
                  mtry=108, 
                  importance=TRUE,
                  ntree=500)
print(rf)
```

# {-}

### Manual Tune using best model above 

Here I am looking at different ntree values while keeping mtry constant.

```{r}
tunegrid <- expand.grid(.mtry=c(sqrt(ncol(x))))
modellist <- list()

# What is the best ntree value 
for (ntree in c(500, 1000, 1500, 2000, 2500)) {
    set.seed(seed)
    fit <- train(outcome_BMI_fnl_BL~., data=dataset[2:328], 
                 method="rf", 
                 metric=metric, 
                 tuneGrid=tunegrid, 
                 trControl=control_default, # this setting performed best above 
                 ntree=ntree)
    key <- toString(ntree)
    modellist[[key]] <- fit
}
# compare results
results <- resamples(modellist)
```

```{r}
summary(results)
dotplot(results)
```

# RF with X Boost {.tabset}

None of the RF models thus far are particularly good or explain all that much variance. 

```{r}
# Create a grid of hyperparameters for the XGBoost model
xgb.grid = expand.grid(
  nrounds = 100,                     # Number of boosting rounds
  eta = seq(0.1, 1, 0.2),            # Learning rate values to try
  max_depth = c(3, 5, 10),           # Depth of trees to try
  gamma = 0,                         # Minimum loss reduction required to make a further partition
  subsample = 0.7,                   # Proportion of samples to use for each tree
  min_child_weight = c(1, 3, 5),     # Minimum sum of instance weight (hessian) needed in a child
  colsample_bytree = 1)              # Proportion of features to use for each tree


# Set up cross-validation control parameters for model training
xgb.control = trainControl(
  method = "cv",                     # Type of resampling (cross-validation)
  number = 10,                       # Number of folds
  repeats = 10,                      # Number of times to repeat cross-validation
  verboseIter = TRUE,                # Print progress messages
  returnData = FALSE,                # Don't return the training data
  returnResamp = "none",             # Don't return resampled data
  classProbs = FALSE,                # Not a classification problem
  allowParallel = TRUE)              # Allow parallel processing
```

```{r}
# Train the XGBoost model using the specified training set and hyperparameter grid
xgb.train = train(
  x = dataset[2:327],                # Predictor variables (feature columns)
  y = dataset$outcome_BMI_fnl_BL,      # Target variable (outcome)
  trControl = xgb.control,           # Control parameters defined above
  tuneGrid = xgb.grid,               # Hyperparameter grid
  method = "xgbTree")                # Specify the XGBoost tree method
```

```{r}
label <- dataset$outcome_BMI_fnl_BL  # Extract the target variable
train_matrix <- as.matrix(dataset[2:327]) # Convert the feature data frame to a matrix

# Set up the parameters for the final model using the best tuning results
params <- list(
  "eta" = xgb.train$bestTune$eta,               # Best learning rate from tuning
  "max_depth" = xgb.train$bestTune$max_depth,   # Best tree depth from tuning
  "gamma" = xgb.train$bestTune$gamma,           # Best gamma from tuning
  "min_child_weight" = xgb.train$bestTune$min_child_weight,  # Best min_child_weight from tuning
  "nthread" = 4,                                 # Number of threads for parallel processing
  "objective" = "reg:squarederror")              # Objective function for regression
```

```{r}
# Perform cross-validation to evaluate the model
xgb.crv <- xgb.cv(
  params = params,                   # Parameters for the XGBoost model
  data = train_matrix,               # Feature data
  label = label,                     # Target variable
  nrounds = 500,                     # Maximum number of boosting rounds
  nfold = 10,                        # Number of folds for cross-validation
  showsd = TRUE,                     # Show standard deviation of evaluation metric
  metrics = "rmse",                  # Evaluation metric (root mean squared error)
  stratified = FALSE,                # Not stratified (used for classification)
  verbose = TRUE,                    # Print progress messages
  print_every_n = 1L,                # Print evaluation every n iterations
  early_stopping_rounds = 50)        # Stop training early if no improvement in the metric
```

```{r}
# Train the final XGBoost model using the best iteration from cross-validation
xgb.mod = xgboost(
  data = train_matrix,                                 # Feature data
  label = label,                                       # Target variable
  max.depth = xgb.train$bestTune$max_depth,            # Best tree depth from tuning
  eta = xgb.train$bestTune$eta,                        # Best learning rate from tuning
  nthread = 4,                                         # Number of threads for parallel processing
  min_child_weight = xgb.train$bestTune$min_child_weight,  # Best min_child_weight from tuning
  # scale_pos_weight = sumwneg/sumwpos,                # Uncomment for class imbalance handling (if needed)
  eval_metric = "rmse",                                # Evaluation metric
  nrounds = xgb.crv$best_iteration,                    # Number of boosting rounds (best iteration)
  objective = "reg:squarederror")                      # Objective function for regression
```

```{r}
importance=xgb.importance(feature_names = colnames(dataset[2:327]), 
                          model = xgb.mod)
importance$Feature[1:5]

# relabel features for the plot 
#feature.label=importance$Feature[1:5]
#feature.label=c("BL Insulin", "BL TGs Lipid", "BL Glucose", "Age", "BL HOMA IR")
```

```{r}
(gg=xgb.ggplot.importance(importance_matrix = importance[1:15,],
                          rel_to_first = FALSE,
                          n_clusters = c(1:3)))
```

Boruta 
Now we can move on to Boruta, feature selection and SHAP. https://amirali-n.github.io/BorutaFeatureSelectionWithShapAnalysis/

```{r}
# Using the Boruta package for feature selection based on the XGBoost algorithm
xgb.boruta=Boruta(dataset[2:327],
                  y=dataset$outcome_BMI_fnl_BL,
                  maxRuns=100, # Maximum number of runs to determine feature importance
                  doTrace=2, # Level of tracing (0: none, 1: basic, 2: detailed)
                  holdHistory=TRUE,
                  getImp=getImpXgboost,
                  max.depth=xgb.train$bestTune$max_depth, 
                  eta=xgb.train$bestTune$eta, 
                  nthread=4, 
                  min_child_weight=xgb.train$bestTune$min_child_weight,#Min sum inst wt (hessian)
                  #scale_pos_weight=sumwneg/sumwpos, 
                  #eval_metric="auc", 
                  eval_metric="rmse", 
                  #eval_metric="logloss",
                  gamma=xgb.train$bestTune$gamma,
                  nrounds=xgb.crv$best_iteration, 
                  objective="reg:squarederror", # Objv funct: regression w/ sqrd error loss
                  tree_method="hist", # Tree construction algorithm
                  lambda=0, # L2 regularization term on weights (default 0)
                  alpha=0) # L1 regularization term on weights (default 0)
```

```{r}
#extract Boruta's decision
boruta_dec=attStats(xgb.boruta)
boruta_dec[boruta_dec$decision!="Rejected",]
```

```{r}
#get the names of each feature
#imp_features=row.names(boruta_dec)[which(boruta_dec$decision!="Rejected")]
#get feature importance history
boruta.imp.df=as.data.frame(xgb.boruta$ImpHistory)
#keep only confirmed and tentative features
#boruta.imp.df=boruta.imp.df[,names(boruta.imp.df)%in%imp_features]
#transform the data to a data frame with two columns: feature and importance value
boruta.imp.df=melt(boruta.imp.df)
```

```{r}
#create a data frame by adding the decision for each feature as well
boruta.imp.df=cbind.data.frame(boruta.imp.df, 
                               decision=boruta_dec$decision[match(boruta.imp.df$variable, 
                                                                 row.names(boruta_dec))])
#reorder features data frame by the importance median value
feature_order=with(boruta.imp.df, 
                   reorder(variable, 
                           value, 
                           median, # Using the median as the criterion for reordering
                           order = TRUE))

# Set the levels of the 'variable' factor in the data frame to reflect the new order
boruta.imp.df$variable=factor(boruta.imp.df$variable, 
                              levels = levels(feature_order)) # Ensures order applied to levels

# Create the box plot
tentative = boruta.imp.df %>% unique %>% 
            filter(!decision == "Rejected")
ggplot(tentative, aes(x = variable, y = value, fill = decision)) +
  geom_boxplot(outlier.shape = NA) +  # Avoid plotting outliers
  coord_flip() +  # Flip coordinates for better readability
  labs(title = "Boruta Importance Evaluation",
       x = "Features",
       y = "Importance Value") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10),  # Adjust text size
        legend.title = element_blank())  # Remove legend title
```

# {-}

## All the models 
```{r}
print(rf$finalModel)
print(rf_default$finalModel)
print(rf_gridsearch$finalModel)
print(rf_random$finalModel)
```


```{r}
# List of models and their names for easy referencing
models <- list(
  rf = rf,
  rf_default = rf_default,
  rf_gridsearch = rf_gridsearch,
  rf_random = rf_random
  #xgb = xgb.mod
)
# Loop through each model
for (model_name in names(models)) {
  model <- models[[model_name]]
  # Get predictions
  test_predictions <- predict(model, newdata = test_path)
  # Calculate performance metrics
  metrics <- postResample(test_predictions, test_path$outcome_BMI_fnl_BL)
  # Print performance metrics
  cat("\nPerformance Metrics for", model_name, ":\n")
  print(metrics)
  # Create a data frame for predicted vs observed
  pred_obs <- data.frame(predicted = test_predictions, 
                         observed = test_path$outcome_BMI_fnl_BL)
  
  # Create the plot
  p <- ggplot(data = pred_obs, aes(x = predicted, y = observed)) + 
    geom_point(size = 5, color = "orange") + 
    xlab("Predicted BL BMI") + 
    ylab("Observed BL BMI") +
    lims(x = c(0, 5), y = c(0, 5)) + 
    geom_abline(linetype = 5, color = "blue", size = 1) + # Perfect fit line
    theme(panel.border = element_rect(colour = "black", fill = NA),
          panel.background = element_blank()) +
    ggtitle(paste("Predicted vs Observed for", model_name)) + # Add a title
    # Add annotations for performance metrics
    annotate("text", x = 4.5, y = 4.5, 
             label = paste("RMSE:", round(metrics[1], 2), "\nR-squared:", round(metrics[2], 2)),
             hjust = 1, vjust = 1, size = 5, color = "black", 
             fontface = "bold", bg = "lightgrey") # Adjust positioning as needed
  print(p)
}
```

# {-}

None of the models perform well 

### RFE feature selection? 

```{r}
control <- rfeControl(functions=rfFuncs, 
                      method="cv", 
                      number=10,
                      repeats = 10,
                      p = 0.80,
                      returnResamp = "final",
                      saveDetails = TRUE,
                      timingSamps = 10)

# run the RFE algorithm
rfe_predictors <- rfe(train_path[,2:327],
                  train_path[,268],
                  #sizes=c(4:13),
                  rfeControl=control)

print(rfe_predictors)
plot(rfe_predictors, type=c("g", "o"))
```
What if I make a linear model with these 5 slected variables. 

```{r}
rfe_linear <- lm(outcome_BMI_fnl_BL ~ taxadiene_biosynthesis_.engineered. + superpathway_of_glycolysis_and_Entner.Doudoroff + superpathway_of_N.acetylneuraminate_degradation + heme_biosynthesis_II_.anaerobic. + D.glucarate_degradation_I, data = dataset)
summary(rfe_linear)
```

Again very poor. 

### Save models
```{r}
rf_models <- list(rf, rf_default, rf_gridsearch, rf_random, xgb.mod)
saveRDS(rf_models, 
        file = "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/functional/aim1/rf_models.rds")
```







