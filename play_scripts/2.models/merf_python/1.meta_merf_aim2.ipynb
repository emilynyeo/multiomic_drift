{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from merf import MERF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "sns.set_context(\"poster\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (11,8)\n",
    "from merf.merf import MERF\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from merf.viz import plot_merf_training_stats\n",
    "from em_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set thresholds\n",
    "max_iter = 400\n",
    "n_est = 100\n",
    "n_js = 1\n",
    "OOB = True\n",
    "mrf = MERF(fixed_effects_model =\n",
    "           RandomForestRegressor(n_estimators = n_est, \n",
    "                                n_jobs = n_js, \n",
    "                                oob_score= True),\n",
    "            gll_early_stop_threshold=None,\n",
    "            max_iterations=max_iter)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "mse_rf = []\n",
    "mse_rfplus = []\n",
    "mse_mrf = []\n",
    "mse_mrf_id = []\n",
    "mse_mrf_id_both = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_plots\"\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"---------- Read metadata ----------\")\n",
    "m1_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/clinical/transformed/aim2\"\n",
    "test = read_data(m1_dir, \"a2_test_samples_standard_clinical.csv\")\n",
    "train = read_data(m1_dir, \"a2_train_samples_standard_clinical.csv\")\n",
    "full = read_data(m1_dir, \"a2_meta_Transformed_standard_clinical.csv\")\n",
    "full_raw = read_data(m1_dir, \"a2_meta_not_Transformed_standard_clinical.csv\")\n",
    "\n",
    "print(full_raw.columns.to_list() == train.columns.to_list())\n",
    "print(train.columns.to_list() == test.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make long format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each meta dataset\n",
    "print(\"---------- Convert metadata to long format ----------\")\n",
    "full_long = make_long(full_raw)\n",
    "full_long['x_t'] = full_long['subject_id'].astype(str) + '.' + full_long['time'].astype(str)\n",
    "\n",
    "train_long = make_long(train)\n",
    "train_long['x_t'] = train_long['subject_id'].astype(str) + '.' + train_long['time'].astype(str)\n",
    "\n",
    "test_long = make_long(test)\n",
    "test_long['x_t'] = test_long['subject_id'].astype(str) + '.' + test_long['time'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train data outcome_BMI_fnl values:\", train_long['outcome_BMI_fnl'])\n",
    "print(\"Full columns after transformation:\", full_long.columns.to_list())\n",
    "print(\"Train columns after transformation:\", train_long.columns.to_list())\n",
    "print(\"Test columns after transformation:\", test_long.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns only if they exist in the DataFrame (since some may not be present after merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'cohort_number', 'record_id', 'x_t']\n",
    "full_long = full_long.drop([col for col in columns_to_drop if col in full.columns], axis=1)\n",
    "train_long = train_long.drop([col for col in columns_to_drop if col in train_long.columns], axis=1)\n",
    "test_long = test_long.drop([col for col in columns_to_drop if col in test_long.columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final columns\n",
    "print(\"Final columns after drop:\", full_long.columns.to_list())\n",
    "print(\"Final test columns after drop:\", test_long.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA \n",
    "test_long = test_long.dropna()\n",
    "train_long = train_long.dropna()\n",
    "full_long = full_long.dropna()\n",
    "raw_train = full_long[full_long['subject_id'].isin(train_long['subject_id'])]\n",
    "raw_test = full_long[full_long['subject_id'].isin(test_long['subject_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- raw train and tax ----------\")\n",
    "print(\"raw_train shape = \", raw_train.shape)\n",
    "print(\"raw_test shape = \", raw_test.shape)\n",
    "print(\"---------- preprocessed train and tax ----------\")\n",
    "print(\"test_long shape = \", test_long.shape)\n",
    "print(\"train_long shape = \", train_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- Select predictors for Basic Raw training set ----------\")\n",
    "train_set = raw_train\n",
    "X = train_set.drop(['outcome_BMI_fnl', 'subject_id'], axis=1)\n",
    "#X = X.drop(columns=['Unnamed: 0_tax', 'x_t'], errors='ignore')\n",
    "Y = train_set[['outcome_BMI_fnl']]\n",
    "Y = Y['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array\n",
    "clusters_train = train_set['subject_id'].to_numpy() # Get ID variables\n",
    "Z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "time = train_set['time'].astype(float).to_numpy() # Get time values as numeric array \n",
    "\n",
    "# Check the final columns\n",
    "print(\"Final columns after drop:\", X.columns.to_list())\n",
    "print(\"X train values:\", train_set['outcome_BMI_fnl'])\n",
    "\n",
    "print(\"---------- Select predictors for Basic Raw test set ----------\")\n",
    "test_set = raw_test\n",
    "X_new = test_set.drop(['outcome_BMI_fnl', 'subject_id'], axis=1)\n",
    "X_new = X_new[X.columns]  # Reorder and select columns to match training set\n",
    "X_new = X_new.astype(X.dtypes)  # Ensure data types match\n",
    "\n",
    "Y_new = test_set['outcome_BMI_fnl'].to_numpy()  # Convert Y to numeric array\n",
    "clusters_new = pd.Series(test_set['subject_id'])  # Convert to pandas Series\n",
    "# Create random effects matrix with ones\n",
    "Z_new = np.ones((len(X_new), 1))\n",
    "time_new = test_set['time'].astype(float).to_numpy()  # Convert time values to numeric array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"---------- RUN MERF RAW BASICðŸŒ± ----------\")\n",
    "# mrf_basic = mrf.fit(X.select_dtypes(include=[np.number]), \n",
    "#         Z, \n",
    "#         pd.Series(clusters_train), \n",
    "#         Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_merf_training_stats(mrf_basic)\n",
    "# plt.savefig(os.path.join(output_dir, '1.clinical/merf_raw_metrics.png'), dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the fitted model\n",
    "# X_new = X_new.drop(columns=['x_t'], errors='ignore')\n",
    "# y_hat_new = mrf_basic.predict(X_new, Z_new, clusters_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print RMSE and R-squared\n",
    "# print(\"---------- Metrics for MERF BASICðŸŒ± ----------\")\n",
    "# calculate_metrics(Y_new, y_hat_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest = mrf_basic.trained_fe_model\n",
    "# oob = str(round(forest.oob_score_*100, 1))  # percent variation\n",
    "# print(f\" % Variation in training set: {oob}\")\n",
    "# print(percent_var_statement(step=\"full_forest\", oob=oob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all the components of forest to a csv file\n",
    "# df = pd.DataFrame.from_dict(forest.__dict__, orient='index')\n",
    "# df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs\"\n",
    "# df.to_csv(os.path.join(df_dir,'merf_basic_raw_forest_components.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R - squared version for mixed model, PREV maybe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of the featur importance function\n",
    "# print(\"---------- Feature importances for MERF BASICðŸŒ± ----------\")\n",
    "# plot_feature_importances(forest.feature_names_in_, \n",
    "#                          forest.feature_importances_, \n",
    "#                          output_dir, 'merf_raw_feature_importances.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_predicted_vs_actual(Y_new, y_hat_new, output_dir, 'clinical_predicted_vs_actual.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- RUN MERF preprocessed with participant RE ðŸŒ±ðŸŒ¸ ----------\")\n",
    " # Mixed Effects Random Forest Training with participant RE and time cluster \n",
    "train_set = train_long \n",
    "X_train = train_set.drop(['outcome_BMI_fnl', 'subject_id', 'time'], axis=1).to_numpy()\n",
    "Z_train = np.array((np.ones(len(train_set)), train_set['subject_id'].apply(lambda s: int(s[-3:])))).T\n",
    "clusters_train = pd.Series(train_set['subject_id'].apply(lambda s: int(s[-3:]))).astype(float)  # Convert to float if necessary\n",
    "y_train = train_set[['outcome_BMI_fnl']]\n",
    "y_train = y_train['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dimensions of X_train: {X_train.shape}\")\n",
    "print(f\"Dimensions of Z_train: {Z_train.shape}\")\n",
    "print(f\"Number of unique inputs for clusters_train: {clusters_train.nunique()}\")\n",
    "print(f\"Inputs to clusters_train: {clusters_train}\")\n",
    "print(f\"Dimensions of y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mrf_id_fe = mrf.fit(X_train, Z_train, clusters_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest = mrf_id_fe.trained_fe_model\n",
    "# oob = str(round(forest.oob_score_*100, 1)) \n",
    "# print(f\" % Variation in training set: {oob}\")\n",
    "# # Save all the components of forest to a csv file\n",
    "# # Convert forest.__dict__ to a DataFrame with a single row\n",
    "# df = pd.DataFrame.from_dict(forest.__dict__, orient='index')\n",
    "# df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs\"\n",
    "# df.to_csv(os.path.join(df_dir,'merf_id_forest_components.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Mixed Effects Random Forests (MERF), the Generalized Log-Likelihood (GLL) is used to evaluate the quality of the model at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_merf_training_stats(mrf_id_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MERF if RE on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data (repeat similar steps)\n",
    "test_set = test_long\n",
    "X_test = test_set.drop(['outcome_BMI_fnl', 'subject_id', 'time'], axis=1).to_numpy()\n",
    "Z_test = np.array((np.ones(len(test_set)), test_set['subject_id'].apply(lambda s: int(s[-3:])))).T\n",
    "clusters_test = pd.Series(test_set['subject_id'].apply(lambda s: int(s[-3:]))).astype(float)  # Convert to float if necessary\n",
    "# Make predictions\n",
    "# yhat_merf = mrf_id_fe.predict(X_test, Z_test, clusters_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print RMSE and R-squared\n",
    "print(\"---------- Metrics for MERF preprocessed with participant RE ðŸŒ±ðŸŒ¸ ----------\")\n",
    "#calculate_metrics(test_set['outcome_BMI_fnl'], yhat_merf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning basic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"---------- MERF with finetuning RE ðŸŒ± ----------\")\n",
    "# import itertools \n",
    "# print(\"---------- Select predictors for Basic Raw training set ----------\")\n",
    "# train_set = raw_train\n",
    "# X = train_set.drop(['outcome_BMI_fnl', 'subject_id', 'x_t'], axis=1)\n",
    "# print(f\"X dimensions: {X.shape}\")\n",
    "# y = train_set[['outcome_BMI_fnl']]\n",
    "# y = y['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array\n",
    "# print(f\"y dimensions: {y.shape}\")\n",
    "# clusters = train_set['subject_id'].to_numpy() # Get ID variables\n",
    "# print(f\"clusters dimensions: {clusters.shape}\")\n",
    "# z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "# print(f\"z dimensions: {z.shape}\")\n",
    "\n",
    "# # Hyperparameters to tune\n",
    "# param_grid = {\n",
    "#     'n_estimators': [1, 100, 300],\n",
    "#     'max_depth': [None, 1, 10],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'max_iter': [2, 30, 10]\n",
    "# }\n",
    "\n",
    "# best_score = float('inf')\n",
    "# best_params = {}\n",
    "\n",
    "# # K-fold cross-validation\n",
    "# kf = KFold(n_splits=10)\n",
    "# total_combinations = len(list(itertools.product(*param_grid.values())))\n",
    "# current_combination = 0\n",
    "# for params in itertools.product(*param_grid.values()):\n",
    "#     current_combination += 1\n",
    "#     print(f\"Processing combination {current_combination}/{total_combinations}: {params}\")\n",
    "#     n_estimators, max_depth, min_samples_split, max_iter = params\n",
    "#     scores = []\n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # Use .iloc for row selection\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#         clusters_train, clusters_test = clusters[train_index], pd.Series(clusters[test_index])\n",
    "#         z_train, z_test = z[train_index], z[test_index]\n",
    "#         #z_train = z[train_index]  # Slice z to match the size of X_train\n",
    "#         # Initialize MERF with current parameters from the grid\n",
    "#         # Initialize MERF with current parameters from the grid\n",
    "#         model = MERF(fixed_effects_model =\n",
    "#                      RandomForestRegressor(n_estimators=n_estimators, \n",
    "#                                           max_depth=max_depth, \n",
    "#                                           min_samples_split=min_samples_split,\n",
    "#                                           n_jobs=n_js, \n",
    "#                                           oob_score=True),\n",
    "#                      gll_early_stop_threshold=None,\n",
    "#                      max_iterations=max_iter)\n",
    "\n",
    "#         #model.fit(X_train, y_train, clusters_train)\n",
    "#         model.fit(X_train.select_dtypes(include=[np.number]), z_train, pd.Series(clusters_train), y_train)\n",
    "#         y_pred = model.predict(X_test, z_test, clusters_test)\n",
    "#         scores.append(np.mean((y_pred - y_test) ** 2)) # MSE\n",
    "\n",
    "#     # Calculate the mean of the scores for the current combination of parameters\n",
    "#     mean_score = np.mean(scores)\n",
    "#     # Check if the mean score is better than the current best score\n",
    "#     if mean_score < best_score:\n",
    "#         # Update the best score and the parameters that achieved it\n",
    "#         best_score = mean_score\n",
    "#         best_params = params\n",
    "\n",
    "# print(\"Best parameters:\", best_params)\n",
    "# print(\"Best score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: (5, 10, 5, 10)\n",
    "Best score: 15.420998172500717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuned_model = MERF(fixed_effects_model =\n",
    "#                 RandomForestRegressor(n_estimators=5, \n",
    "#                                         max_depth=10, \n",
    "#                                         min_samples_split=5,\n",
    "#                                         n_jobs=n_js, \n",
    "#                                         oob_score=True),\n",
    "#                      gll_early_stop_threshold=None,\n",
    "#                      max_iterations=10)\n",
    "\n",
    "#         #model.fit(X_train, y_train, clusters_train)\n",
    "# tuned_model.fit(X_train.select_dtypes(include=[np.number]), z_train, pd.Series(clusters_train), y_train)\n",
    "# y_pred = tuned_model.predict(X_test, z_test, clusters_test)\n",
    "# mse = ((y_pred - y_test) ** 2) # MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 100 iteractions: \n",
    "Correlation between actual and predicted values: 0.3893\n",
    "Root Mean Squared Error: 3.0546\n",
    "R-squared Score: 0.1046\n",
    "\n",
    "With 10 iterations:\n",
    "Correlation between actual and predicted values: 0.4391\n",
    "Root Mean Squared Error: 2.9560\n",
    "R-squared Score: 0.1614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_predicted_vs_actual(y_test, y_pred, output_dir, 'fine_tuned_actual_vs_predicted_10.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest = tuned_model.trained_fe_model\n",
    "# feature_names = forest.feature_names_in_\n",
    "# feature_importances = forest.feature_importances_\n",
    "# plot_feature_importances(feature_names, feature_importances, output_dir, 'tuned_feature_importances_clinical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning wth PREV and PTEV.\n",
    "ptev is calculated as the ratio of fixed effect variance to total variance, while prev is calculated as the ratio of random effect variance to total variance.\n",
    "\n",
    "A higher PTEV is often preferable because it suggests that your model explains a larger portion of the observed variability. However, excessively high PTEV could signal overfitting, where the model memorizes rather than generalizes.\n",
    "\n",
    "The optimal PREV depends on your data and goals. If your model is designed to capture group- or subject-level variability, a higher PREV might be desirable. However, in most cases, excessively high PREV might indicate the model isn't capturing key fixed effects, potentially leading to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- MERF with finetuning RE ðŸŒ± ----------\")\n",
    "import itertools \n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory where the CSV file will be saved\n",
    "df_dir = 'path/to/your/directory'\n",
    "\n",
    "print(\"---------- Select predictors for Basic Raw training set ----------\")\n",
    "train_set = raw_train\n",
    "X = train_set.drop(['outcome_BMI_fnl', 'subject_id', 'x_t'], axis=1)\n",
    "print(f\"X dimensions: {X.shape}\")\n",
    "y = train_set[['outcome_BMI_fnl']]\n",
    "y = y['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array\n",
    "print(f\"y dimensions: {y.shape}\")\n",
    "clusters = train_set['subject_id'].to_numpy() # Get ID variables\n",
    "print(f\"clusters dimensions: {clusters.shape}\")\n",
    "z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "print(f\"z dimensions: {z.shape}\")\n",
    "\n",
    "# # Hyperparameters to tune\n",
    "# param_grid = {\n",
    "#     'n_estimators': [1, 100, 300],\n",
    "#     'max_depth': [None, 1, 100],\n",
    "#     'min_samples_split': [2, 10],\n",
    "#     'max_iter': [2, 30, 100]\n",
    "# }\n",
    "\n",
    "# best_score = float('inf')\n",
    "# best_params = {}\n",
    "\n",
    "# # K-fold cross-validation\n",
    "# kf = KFold(n_splits=10)\n",
    "# total_combinations = len(list(itertools.product(*param_grid.values())))\n",
    "# current_combination = 0\n",
    "\n",
    "# # Initialize a list to store the results of each iteration\n",
    "# results = []\n",
    "\n",
    "# for params in itertools.product(*param_grid.values()):\n",
    "#     current_combination += 1\n",
    "#     print(f\"Processing combination {current_combination}/{total_combinations}: {params}\")\n",
    "#     n_estimators, max_depth, min_samples_split, max_iter = params\n",
    "#     scores = []\n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # Use .iloc for row selection\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#         clusters_train, clusters_test = clusters[train_index], pd.Series(clusters[test_index])\n",
    "#         z_train, z_test = z[train_index], z[test_index]\n",
    "\n",
    "#         model = MERF(fixed_effects_model =\n",
    "#                      RandomForestRegressor(n_estimators=n_estimators, \n",
    "#                                           max_depth=max_depth, \n",
    "#                                           min_samples_split=min_samples_split,\n",
    "#                                           n_jobs=n_js, \n",
    "#                                           oob_score=True),\n",
    "#                      gll_early_stop_threshold=None,\n",
    "#                      max_iterations=max_iter)\n",
    "\n",
    "#         model.fit(X_train.select_dtypes(include=[np.number]), z_train, pd.Series(clusters_train), y_train)\n",
    "#         y_pred = model.predict(X_test, z_test, clusters_test)\n",
    "#         scores.append(np.mean((y_pred - y_test) ** 2)) # MSE\n",
    "\n",
    "#         # Calculate ptev and prev\n",
    "#         total_variance = np.var(y_test) #calculates the total variance of the predicted values\n",
    "#         random_effect_variance = np.var(y_test - y_pred)  # Variance of residuals\n",
    "#         fixed_effect_variance = total_variance - random_effect_variance\n",
    "\n",
    "#         ptev.append(np.mean(fixed_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "#         prev.append(np.mean(random_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "\n",
    "#         # Print ptev and prev for the current iteration\n",
    "#         print(f\"Combination {current_combination}, ptev: {np.mean(ptev):.4f}, prev: {np.mean(prev):.4f}\")\n",
    "\n",
    "#         # Calculate the mean of the scores for the current combination of parameters\n",
    "#         mean_score = np.mean(scores)\n",
    "#         mean_prev = np.mean(prev)\n",
    "#         mean_ptev = np.mean(ptev)\n",
    "#         if mean_score < best_score:\n",
    "#             best_score = mean_score\n",
    "#             best_params = params\n",
    "\n",
    "#         # Append the results of the current iteration to the results list\n",
    "#         # Create a result dictionary with individual scores and mean scores\n",
    "#         result_dict = {\n",
    "#             'n_estimators': n_estimators,\n",
    "#             'max_depth': max_depth,\n",
    "#             'min_samples_split': min_samples_split,\n",
    "#             'max_iter': max_iter,\n",
    "#             'mse_score_1': scores[0] if len(scores) > 0 else None,\n",
    "#             'mse_score_2': scores[1] if len(scores) > 1 else None,\n",
    "#             'mse_score_3': scores[2] if len(scores) > 2 else None,\n",
    "#             'mse_score_4': scores[3] if len(scores) > 3 else None,\n",
    "#             'mse_score_5': scores[4] if len(scores) > 4 else None,\n",
    "#             'mse_score_6': scores[5] if len(scores) > 5 else None,\n",
    "#             'mse_score_7': scores[6] if len(scores) > 6 else None,\n",
    "#             'mse_score_8': scores[7] if len(scores) > 7 else None,\n",
    "#             'mse_score_9': scores[8] if len(scores) > 8 else None,\n",
    "#             'mse_score_10': scores[9] if len(scores) > 9 else None,\n",
    "#             'mean_mse_score': mean_score,\n",
    "#             'prev_1': prev[0] if len(prev) > 0 else None,\n",
    "#             'prev_2': prev[1] if len(prev) > 1 else None,\n",
    "#             'prev_3': prev[2] if len(prev) > 2 else None,\n",
    "#             'prev_4': prev[3] if len(prev) > 3 else None,\n",
    "#             'prev_5': prev[4] if len(prev) > 4 else None,\n",
    "#             'prev_6': prev[5] if len(prev) > 5 else None,\n",
    "#             'prev_7': prev[6] if len(prev) > 6 else None,\n",
    "#             'prev_8': prev[7] if len(prev) > 7 else None,\n",
    "#             'prev_9': prev[8] if len(prev) > 8 else None,\n",
    "#             'prev_10': prev[9] if len(prev) > 9 else None,\n",
    "#             'mean_prev': mean_prev,\n",
    "#             'ptev_1': ptev[0] if len(ptev) > 0 else None,\n",
    "#             'ptev_2': ptev[1] if len(ptev) > 1 else None,\n",
    "#             'ptev_3': ptev[2] if len(ptev) > 2 else None,\n",
    "#             'ptev_4': ptev[3] if len(ptev) > 3 else None,\n",
    "#             'ptev_5': ptev[4] if len(ptev) > 4 else None,\n",
    "#             'ptev_6': ptev[5] if len(ptev) > 5 else None,\n",
    "#             'ptev_7': ptev[6] if len(ptev) > 6 else None,\n",
    "#             'ptev_8': ptev[7] if len(ptev) > 7 else None,\n",
    "#             'ptev_9': ptev[8] if len(ptev) > 8 else None,\n",
    "#             'ptev_10': ptev[9] if len(ptev) > 9 else None,\n",
    "#             'mean_ptev': mean_ptev\n",
    "#         }\n",
    "#         # Append the result dictionary to the results list\n",
    "#         results.append(result_dict)\n",
    "\n",
    "# print(\"Best parameters:\", best_params)\n",
    "# print(\"Best score:\", best_score)\n",
    "\n",
    "# # Convert the results list to a DataFrame and save it to a CSV file\n",
    "# df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/1.clinical\"\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(f'{df_dir}/fine_tuning_results_raw_clinical.csv', index=False)\n",
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [1, 5, 100, 300],\n",
    "    'max_depth': [None, 1, 50, 100],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'max_iter': [2, 3, 10, 20]\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Initialize a list to store the results of each iteration\n",
    "results = []\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=10)\n",
    "total_combinations = len(list(itertools.product(*param_grid.values())))\n",
    "current_combination = 0\n",
    "for params in itertools.product(*param_grid.values()):\n",
    "        current_combination += 1\n",
    "        print(f\"Combination {current_combination}/{total_combinations}: {params}\\n\")\n",
    "        n_estimators, max_depth, min_samples_split, max_iter = params\n",
    "        scores = []\n",
    "        prev = []\n",
    "        ptev = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # Use .iloc for row selection\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clusters_train, clusters_test = clusters[train_index], pd.Series(clusters[test_index])\n",
    "            z_train, z_test = z[train_index], z[test_index]\n",
    "            model = MERF(\n",
    "                # Specify the fixed effects model as a Random Forest Regressor\n",
    "                fixed_effects_model=RandomForestRegressor(\n",
    "                    n_estimators=n_estimators,  # Number of trees in the forest\n",
    "                    max_depth=max_depth,  # Maximum depth of each tree\n",
    "                    min_samples_split=min_samples_split,  # Minimum samples required to split an internal node\n",
    "                    n_jobs=1,  # Number of jobs to run in parallel\n",
    "                    oob_score=True  # Whether to use out-of-bag samples to estimate the R^2 on unseen data\n",
    "                ),\n",
    "                # Generalized Linear Model (GLM) early stopping threshold\n",
    "                gll_early_stop_threshold=None,  # No early stopping threshold set\n",
    "                # Maximum number of iterations for the MERF algorithm\n",
    "                max_iterations=max_iter  # Maximum number of iterations to run the MERF algorithm\n",
    "            )\n",
    "            #model.fit(X_train, y_train, clusters_train)\n",
    "            model.fit(X_train.select_dtypes(include=[np.number]), z_train, pd.Series(clusters_train), y_train)\n",
    "            y_pred = model.predict(X_test, z_test, clusters_test)\n",
    "            scores.append(np.mean((y_pred - y_test) ** 2)) # MSE\n",
    "            \n",
    "            # Calculate ptev and prev\n",
    "            total_variance = np.var(y_test) #calculates the total variance of the predicted values\n",
    "            random_effect_variance = np.var(y_test - y_pred)  # Variance of residuals\n",
    "            fixed_effect_variance = total_variance - random_effect_variance\n",
    "\n",
    "            ptev.append(np.mean(fixed_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "            prev.append(np.mean(random_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "\n",
    "            # Print ptev and prev for the current iteration\n",
    "            print(f\"Combination {current_combination}, ptev: {np.mean(ptev):.4f}, prev: {np.mean(prev):.4f}\")\n",
    "\n",
    "        # Calculate the mean of the scores for the current combination of parameters\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_prev = np.mean(prev)\n",
    "        mean_ptev = np.mean(ptev)\n",
    "        if mean_score < best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "\n",
    "        # Append the results of the current iteration to the results list\n",
    "        # Create a result dictionary with individual scores and mean scores\n",
    "        result_dict = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'max_iter': max_iter,\n",
    "            'mse_score_1': scores[0] if len(scores) > 0 else None,\n",
    "            'mse_score_2': scores[1] if len(scores) > 1 else None,\n",
    "            'mse_score_3': scores[2] if len(scores) > 2 else None,\n",
    "            'mse_score_4': scores[3] if len(scores) > 3 else None,\n",
    "            'mse_score_5': scores[4] if len(scores) > 4 else None,\n",
    "            'mse_score_6': scores[5] if len(scores) > 5 else None,\n",
    "            'mse_score_7': scores[6] if len(scores) > 6 else None,\n",
    "            'mse_score_8': scores[7] if len(scores) > 7 else None,\n",
    "            'mse_score_9': scores[8] if len(scores) > 8 else None,\n",
    "            'mse_score_10': scores[9] if len(scores) > 9 else None,\n",
    "            'mean_mse_score': mean_score,\n",
    "            'prev_1': prev[0] if len(prev) > 0 else None,\n",
    "            'prev_2': prev[1] if len(prev) > 1 else None,\n",
    "            'prev_3': prev[2] if len(prev) > 2 else None,\n",
    "            'prev_4': prev[3] if len(prev) > 3 else None,\n",
    "            'prev_5': prev[4] if len(prev) > 4 else None,\n",
    "            'prev_6': prev[5] if len(prev) > 5 else None,\n",
    "            'prev_7': prev[6] if len(prev) > 6 else None,\n",
    "            'prev_8': prev[7] if len(prev) > 7 else None,\n",
    "            'prev_9': prev[8] if len(prev) > 8 else None,\n",
    "            'prev_10': prev[9] if len(prev) > 9 else None,\n",
    "            'mean_prev': mean_prev,\n",
    "            'ptev_1': ptev[0] if len(ptev) > 0 else None,\n",
    "            'ptev_2': ptev[1] if len(ptev) > 1 else None,\n",
    "            'ptev_3': ptev[2] if len(ptev) > 2 else None,\n",
    "            'ptev_4': ptev[3] if len(ptev) > 3 else None,\n",
    "            'ptev_5': ptev[4] if len(ptev) > 4 else None,\n",
    "            'ptev_6': ptev[5] if len(ptev) > 5 else None,\n",
    "            'ptev_7': ptev[6] if len(ptev) > 6 else None,\n",
    "            'ptev_8': ptev[7] if len(ptev) > 7 else None,\n",
    "            'ptev_9': ptev[8] if len(ptev) > 8 else None,\n",
    "            'ptev_10': ptev[9] if len(ptev) > 9 else None,\n",
    "            'mean_ptev': mean_ptev\n",
    "        }\n",
    "        \n",
    "        # Append the result dictionary to the results list\n",
    "        results.append(result_dict)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "\n",
    "# Convert the results list to a DataFrame and save it to a CSV file\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/1.clinical\"\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'{df_dir}/fine_tuning_results_raw_clinical.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results list to a DataFrame and save it to a CSV file\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/1.clinical\"\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'{df_dir}/fine_tuning_results_raw_clinical_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: (100, 50, 5, 10)\n",
      "Best score: 15.267048924651743\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
