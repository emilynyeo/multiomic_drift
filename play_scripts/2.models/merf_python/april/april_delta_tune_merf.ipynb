{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from merf import MERF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "sns.set_context(\"poster\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (11,8)\n",
    "from merf.merf import MERF\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from merf.viz import plot_merf_training_stats\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "current_dir = os.getcwd() # Get the current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '../'))\n",
    "sys.path.append(parent_dir)\n",
    "from em_utils import * # import the utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "# output_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/april/new_split\"\n",
    "output_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/may_basic_plus/delta/\"\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/may_basic_plus/merf_dfs_delta/\"\n",
    "os.makedirs(df_dir, exist_ok=True)\n",
    "\n",
    "print(\"---------- Read data ----------\")\n",
    "input_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/april_processing/\"\n",
    "delta = read_data(input_dir, \"all_delta_april29.csv\")\n",
    "print(delta.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make time numeric \n",
    "time_mapping = {\n",
    "    \"BL\": 0,\n",
    "    \"6m\": 6,\n",
    "    \"12m\": 12\n",
    "}\n",
    "delta['time'] = delta['time'].replace(time_mapping)\n",
    "print(delta['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make omic subsets\n",
    "BMI_VAR, ID_VAR, TIME_VAR, DATA = 'outcome_BMI_fnl', 'subject_id', 'time', delta\n",
    "# Define the column names based on your lists\n",
    "basic = [BMI_VAR, ID_VAR, TIME_VAR, 'sex', 'age', 'randomized_group']\n",
    "meta_keep = [BMI_VAR, ID_VAR, TIME_VAR, 'randomized_group', 'sex', 'race', \n",
    "             'age', 'Glucose.x', 'HOMA_IR', 'Insulin_endo', 'HDL_Total_Direct_lipid', 'LDL_Calculated', 'Triglyceride_lipid']\n",
    "only_grs = [BMI_VAR, ID_VAR, TIME_VAR, 'bmi_prs']\n",
    "only_taxa = [BMI_VAR, ID_VAR, TIME_VAR,] + [col for col in DATA.columns if col.startswith(\"g__\")]\n",
    "\n",
    "micom_start = DATA.columns.get_loc(\"Diacetyl\")\n",
    "micom_end = DATA.columns.get_loc(\"aldehydo.D.xylose\")\n",
    "only_micom = [BMI_VAR, ID_VAR, TIME_VAR,] + list(DATA.columns[micom_start:micom_end + 1])\n",
    "\n",
    "path_start = DATA.columns.get_loc(\"arginine..ornithine.and.proline.interconversion\")\n",
    "path_end = DATA.columns.get_loc(\"UDP.N.acetyl.D.glucosamine.biosynthesis.I\")\n",
    "only_pathway = [BMI_VAR, ID_VAR, TIME_VAR,] + list(DATA.columns[path_start:path_end + 1])\n",
    "\n",
    "metabo_start = DATA.columns.get_loc(\"non_HDL_C\")\n",
    "metabo_end = DATA.columns.get_loc(\"IDL_TG_pct\")\n",
    "only_metabo = [BMI_VAR, ID_VAR, TIME_VAR,] + list(DATA.columns[metabo_start:metabo_end + 1])\n",
    "\n",
    "all_col = [BMI_VAR, ID_VAR, TIME_VAR,] + ['randomized_group', 'sex', 'race',\n",
    "    'age','Glucose.x', 'HOMA_IR', 'Insulin_endo', 'HDL_Total_Direct_lipid', 'LDL_Calculated', 'Triglyceride_lipid'] + \\\n",
    "    list(DATA.columns[DATA.columns.str.startswith(\"g__\")]) + \\\n",
    "    list(DATA.columns[micom_start:micom_end + 1]) + \\\n",
    "    list(DATA.columns[path_start:path_end + 1]) + \\\n",
    "    list(DATA.columns[metabo_start:metabo_end + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and test sets \n",
    "# test sample names\n",
    "#test_names = [\"ABR-079\", \"AGA-071\", \"AHE-055\", \"ALI-121\", \"ALO-163\", \"AMA-031\", \"ASO-013\", \"AWI-167\", \"BMO-164\", \"CWA-183\", \"DSC-024\", \"EBE-130\", \"EHI-177\", \"EJO-092\", \"GFU-188\", \"HGI-010\", \"JCA-109\", \"JGO-100\",\n",
    "#    \"KBU-085\", \"KCE-034\", \"KHE-170\", \"LDO-148\", \"LST-186\", \"LZD-142\", \"MAR-119\", \"MCA-088\", \"MJA-153\", \"MWE-112\", \"NPO-149\", \"RAE-114\", \"SBO-020\", \"SEG-080\", \"SKA-195\", \"SLO-178\", \"SSH-028\", \"TDU-086\",\"TFA-016\", \"VCA-041\"]\n",
    "test_names = [\"ASO-013\", \"NTA-021\", \"KGI-029\", \"KPA-042\", \"AWA-052\", \"AHE-055\", \"COW-066\", \"NBI-069\", \"CEL-073\", \"CAL-074\", \"ABR-079\", \"SEG-080\", \"NKA-090\", \"NEL-094\", \"LJA-101\", \"ADA-105\", \"MLU-106\", \"MDI-107\", \"JER-110\", \"TRO-113\", \"MFB-118\", \"ALI-121\", \"KWA-122\", \"RAF-125\", \"EBE-130\", \"CGA-134\", \"LZD-142\", \"NPO-149\", \"HDE-154\", \"AMC-155\", \"SAB-160\", \"QNG-166\", \"NCO-171\", \"BSA-174\", \"EHI-177\", \"LST-186\", \"MBA-187\", \"BAN-193\"]\n",
    "# train sample names\n",
    "#train_names = [\"AAL-144\", \"ACO-053\", \"ADA-105\", \"AKE-009\", \"AKI-011\", \"AKO-139\", \"AMC-155\", \"AME-128\", \"AME-157\", \"ATA-129\", \"AWA-052\", \"AWA-083\", \"BAN-193\", \"BHO-014\", \"BIN-201\", \"BKN-104\", \"BMI-156\", \"BSA-174\", \"CAM-057\", \"CCO-189\",\n",
    "#    \"CED-026\", \"CEL-073\", \"CGA-134\", \"CIS-077\", \"CKR-078\", \"CLE-049\", \"COW-066\", \"CRO-108\", \"CWA-161\", \"EBE-051\", \"EKA-135\", \"EKR-045\", \"ELA-159\", \"EPO-182\", \"EVO-184\", \"FWI-098\", \"GHA-035\", \"HDE-154\", \"IBE-120\", \"JDI-140\", \"JER-110\", \"JFU-027\", \"JJO-093\", \"JKN-127\", \"JPO-022\", \"JUG-116\", \"JUT-032\", \"JVE-126\", \"KAN-138\", \"KBR-162\", \"KEL-185\", \"KEL-199\", \"KGI-029\", \"KHU-196\", \"KPA-042\", \"KRI-072\", \"KVA-038\", \"KWA-122\", \"KWA-141\", \"LBL-047\", \"LBU-015\", \"LEL-147\", \"LFI-003\", \"LJA-101\", \"LMC-111\", \"LPF-198\", \"LVA-017\", \"MBA-187\", \"MCW-065\", \"MDI-107\", \"MES-068\", \"MFB-118\", \"MGA-076\", \"MHO-117\", \"MKE-192\", \"MMA-036\", \"MRT-179\", \"MSH-091\", \"MST-039\", \"MWE-143\",\n",
    "#    \"MWO-133\", \"MWY-152\", \"NAR-099\", \"NBI-048\", \"NBI-069\", \"NCO-171\", \"NDI-067\", \"NEL-094\", \"NKA-090\", \"NMO-151\", \"NTA-021\", \"PBE-123\", \"QNG-166\", \"RAF-125\", \"RAM-050\", \"RHP-023\", \"RLA-132\", \"ROL-006\", \"SAB-160\", \"SCA-043\", \"SCR-061\", \"SDA-150\", \"SGA-062\", \"SKA-087\", \"SRO-194\", \"TBU-115\", \"TFA-172\", \"TRO-113\", \"TSH-146\", \"TSL-056\", \"WPE-005\", \"YOR-103\", \"YSU-097\", \"ZVU-096\"]\n",
    "train_names = [\"SDA-150\", \"LBU-015\", \"CIS-077\", \"ATA-129\", \"KHU-196\", \"MWY-152\", \"AGA-071\", \"AME-157\", \"CWA-183\", \"RHP-023\", \"MST-025\", \"SSH-028\", \"JUG-116\", \"EJO-092\", \"VCA-041\", \"NMO-151\", \"BHO-014\", \"KBU-085\", \"SBO-020\", \"MWO-133\", \"KRI-072\", \"AAL-144\", \"ALO-163\", \"AKI-011\", \"MHO-117\", \"TSH-146\", \"RAE-114\", \"FWI-098\", \"MAR-119\", \"JGO-100\", \"CAM-057\", \"YOR-103\", \"HGI-010\", \"KAN-138\", \"SGA-062\", \"CKR-078\", \"MWE-112\", \"ROL-006\", \"MMA-036\", \"DSC-024\", \"LDO-148\", \"MCA-088\", \"CPU-075\", \"AKO-139\", \"LFI-003\", \"KWA-141\", \"GFU-188\", \"BMO-164\", \"JPO-022\", \"EVO-184\", \"LPF-198\", \"TBU-115\", \"SRO-194\", \"KEL-199\", \"JFU-027\", \"SKA-195\", \"IBE-120\", \"TSL-056\", \"NDI-067\", \"AWA-083\", \"CWA-161\", \"TDU-086\", \"JCA-109\", \"CBO-004\", \"NAR-099\", \"MES-068\", \"AMA-031\", \"SLO-178\", \"SCA-043\", \"AWI-167\",  \"KBR-162\", \"TFA-172\", \"BIN-201\", \"NBI-048\", \"KHE-170\", \"CSH-012\", \"BMI-156\", \"MWE-143\", \"EKA-135\", \"WPE-005\", \"AKE-009\", \"YSU-097\", \"MCW-065\", \"EBE-051\", \"ZVU-096\", \"JJO-093\", \"KVA-038\", \"ACO-053\", \"RLA-132\", \"MBA-176\", \"CED-026\", \"JDI-140\", \"CCO-189\", \"EKR-045\", \"MJA-153\", \"CLE-049\", \"LMC-111\", \"SKA-087\", \"JUT-032\", \"MKE-192\", \"JVE-126\", \"KCE-034\", \"KEL-185\", \"MRT-179\", \"JKN-127\", \"LEL-147\", \"BKN-104\", \"AME-128\", \"MSH-091\", \"MGA-076\", \"LVA-017\", \"EPO-182\"]\n",
    "\n",
    "print(\"Length of test names:\", len(test_names))\n",
    "print(\"Length of train names:\", len(train_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make summary table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA.columns)\n",
    "#print(delta.columns)\n",
    "print([ID_VAR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sets = {\n",
    "    \"basic\": basic,\n",
    "    \"meta_keep\": meta_keep,\n",
    "    \"only_grs\": only_grs, \n",
    "    \"only_taxa\": only_taxa,\n",
    "    \"only_micom\": only_micom,\n",
    "    \"only_pathway\": only_pathway,\n",
    "    \"only_metabo\" : only_metabo,\n",
    "    \"only_all\" : all_col\n",
    "}\n",
    "\n",
    "# Initialize a list to store summary data\n",
    "summary_data = []\n",
    "\n",
    "# Loop through each column set and create DATA\n",
    "for key, columns in column_sets.items():\n",
    "    DATA = delta[columns]\n",
    "    print(f\"Subset for {key} created with shape: {DATA.shape}\")\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary = DATA.groupby('time').agg(\n",
    "    unique_id_count=(ID_VAR, 'nunique'),  # Count unique ID_VAR\n",
    "    feature_count=('time', lambda x: DATA.shape[1] - 2)  # Count total features excluding 'time' and 'ID_VAR'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Print the summary table\n",
    "    print(\"\\nSummary Table for\", key)\n",
    "    print(summary.to_string(index=False))  # Print without the index\n",
    "    \n",
    "    # Append summary to the summary_data list\n",
    "    summary_data.append((key, summary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sets = {\n",
    "    \"basic\": basic,\n",
    "    \"meta_keep\": meta_keep,\n",
    "    \"only_grs\": only_grs, \n",
    "    \"only_taxa\": only_taxa,\n",
    "    \"only_micom\": only_micom,\n",
    "    \"only_pathway\": only_pathway,\n",
    "    \"only_metabo\": only_metabo,\n",
    "    \"only_all\": all_col\n",
    "}\n",
    "\n",
    "for key, columns in column_sets.items():\n",
    "    print(f\"\\n=== Processing column set: {key} ===\")\n",
    "    DATA = delta[columns]\n",
    "    print(f\"Subset for {key} created with shape: {DATA.shape}\")\n",
    "\n",
    "    OUT = f\"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/may_basic_plus/delta/{key}_delta.csv\"\n",
    "    output_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/may_basic_plus/delta/\"\n",
    "    r2_out = f\"{key}_delta_r2.pdf\"\n",
    "    r2_adj_out = f\"{key}_delta_r2_adj.pdf\"\n",
    "    feature_imp_out = f\"{key}_delta_ft_imp.pdf\"\n",
    "    results_filename = f\"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/may_basic_plus/{key}_delta_april29.csv\"\n",
    "\n",
    "    # Train/test split\n",
    "    train_set = DATA[DATA[ID_VAR].isin(train_names)]\n",
    "    test_set = DATA[DATA[ID_VAR].isin(test_names)]\n",
    "\n",
    "    clusters_train = pd.Series(train_set[ID_VAR])\n",
    "    clusters_test = pd.Series(test_set[ID_VAR])\n",
    "\n",
    "    print(\"---------- Select predictors for training set ----------\")\n",
    "    X = train_set.drop([BMI_VAR, ID_VAR], axis=1)\n",
    "    Y = train_set[BMI_VAR].to_numpy()\n",
    "    Z = np.ones((train_set.shape[0], 1))\n",
    "\n",
    "    print(f\"Length of X: {len(X)}, Length of clusters_train: {len(clusters_train)}, Length of Y: {len(Y)}\")\n",
    "    assert len(X) == len(clusters_train)\n",
    "    assert len(X) == len(Y)\n",
    "    print(\"Final columns after drop:\", X.columns.to_list())\n",
    "\n",
    "    print(\"---------- Select predictors for test set ----------\")\n",
    "    X_new = test_set.drop([BMI_VAR, ID_VAR], axis=1)\n",
    "    X_new = X_new[X.columns]  # align with training set columns\n",
    "    X_new = X_new.astype(X.dtypes)\n",
    "    Y_new = test_set[BMI_VAR].to_numpy()\n",
    "    clusters_new = pd.Series(test_set[ID_VAR])\n",
    "    Z_new = np.ones((len(X_new), 1))\n",
    "    time_new = test_set[TIME_VAR].astype(float).to_numpy()\n",
    "\n",
    "    # Hyperparameters\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [None],\n",
    "        'min_samples_split': [0.05, 0.1, 0.15],\n",
    "        'max_iter': [2, 10],\n",
    "        'n_splits': [3, 5, 10]\n",
    "    }\n",
    "\n",
    "    y = train_set[[BMI_VAR]][BMI_VAR].to_numpy()\n",
    "    clusters = train_set[ID_VAR].to_numpy()\n",
    "    z = np.ones((train_set.shape[0], 1))\n",
    "\n",
    "    best_score = float('inf')\n",
    "    best_params = {}\n",
    "    results = []\n",
    "\n",
    "    total_combinations = len(list(itertools.product(*param_grid.values())))\n",
    "    for idx, params in enumerate(itertools.product(*param_grid.values())):\n",
    "        n_estimators, max_depth, min_samples_split, max_iter, n_splits = params\n",
    "        print(f\"Combination: {params}\")\n",
    "        print(f\"Progress: {(idx + 1) / total_combinations * 100:.2f}%\")\n",
    "\n",
    "        scores, prev, ptev, oob_scores = [], [], [], []\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clusters_train_cv = clusters[train_index]\n",
    "            clusters_test_cv = pd.Series(clusters[test_index])\n",
    "            z_train, z_test = z[train_index], z[test_index]\n",
    "\n",
    "            model = MERF(\n",
    "                fixed_effects_model=RandomForestRegressor(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_split=min_samples_split,\n",
    "                    n_jobs=1,\n",
    "                    oob_score=True\n",
    "                ),\n",
    "                gll_early_stop_threshold=None,\n",
    "                max_iterations=max_iter\n",
    "            )\n",
    "\n",
    "            model.fit(X_train.select_dtypes(include=[np.number]), z_train, pd.Series(clusters_train_cv), y_train)\n",
    "            y_pred = model.predict(X_test, z_test, clusters_test_cv)\n",
    "            mse = np.mean((y_pred - y_test) ** 2)\n",
    "            scores.append(mse)\n",
    "\n",
    "            total_variance = np.var(y_test)\n",
    "            random_effect_variance = np.var(y_test - y_pred)\n",
    "            fixed_effect_variance = total_variance - random_effect_variance\n",
    "\n",
    "            ptev.append(fixed_effect_variance / total_variance if total_variance > 0 else 0)\n",
    "            prev.append(random_effect_variance / total_variance if total_variance > 0 else 0)\n",
    "\n",
    "            forest = model.trained_fe_model\n",
    "            oob_score = round(forest.oob_score_ * 100, 1)\n",
    "            oob_scores.append(oob_score)\n",
    "\n",
    "            print(f\"ptev: {np.mean(ptev):.4f}, prev: {np.mean(prev):.4f}, OOB Score: {oob_score:.4f}\")\n",
    "\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score < best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "\n",
    "        results.append({\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'max_iter': max_iter,\n",
    "            'n_splits': n_splits,\n",
    "            'mean_mse_score': mean_score,\n",
    "            'mean_prev': np.mean(prev),\n",
    "            'mean_ptev': np.mean(ptev),\n",
    "            'oob_score': np.mean(oob_scores)\n",
    "        })\n",
    "\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    print(\"Best score:\", best_score)\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(OUT, index=False)\n",
    "\n",
    "    print(\"---------- Run MERF models ----------\")\n",
    "    r2_values, results_df = run_merf_analysis2(\n",
    "        X, Y, Z, clusters,\n",
    "        X_new, Y_new, Z_new, clusters_new,\n",
    "        results_df,\n",
    "        output_dir, r2_out, r2_adj_out, \n",
    "        feature_imp_out, results_filename, time_new\n",
    "    )\n",
    "\n",
    "    print(\"R-squared values:\", r2_values)\n",
    "    print(\"Results DataFrame:\\n\", results_df)\n",
    "    print(f\"---------- Done saving Merf output for {key} ----------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sets = {\n",
    "    \"basic\": basic,\n",
    "    \"meta_keep\": meta_keep,\n",
    "    \"only_grs\": only_grs, \n",
    "    \"only_taxa\": only_taxa,\n",
    "    \"only_micom\": only_micom,\n",
    "    \"only_pathway\": only_pathway,\n",
    "    \"only_metabo\" : only_metabo,\n",
    "    \"only_all\" : all_col\n",
    "}\n",
    "\n",
    "for key, columns in column_sets.items():\n",
    "    DATA = delta[columns]\n",
    "    print(f\"Subset for {key} created with shape: {DATA.shape}\")\n",
    "    \n",
    "    OUT = f\"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/may_basic_plus/{key}_delta.csv\"\n",
    "    output_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/may_basic_plus/\"\n",
    "    r2_out =  f\"{key}_delta_r2.pdf\"\n",
    "    r2_adj_out = f\"{key}_delta_r2_adj.pdf\"\n",
    "    feature_imp_out = f\"{key}_delta_ft_imp.pdf\"\n",
    "    results_filename = f\"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/may_basic_plus/{key}_delta_april29.csv\"\n",
    "\n",
    "    # Create train and test sets for the current subset\n",
    "train_set = DATA[DATA[ID_VAR].isin(train_names)]  # Select rows where ID_VAR is in train_names\n",
    "test_set = DATA[DATA[ID_VAR].isin(test_names)]    # Select rows where ID_VAR is in test_names\n",
    "\n",
    "# Ensure clusters_train and clusters_test are pandas Series\n",
    "clusters_train = pd.Series(train_set[ID_VAR])  # Convert to pandas Series\n",
    "clusters_test = pd.Series(test_set[ID_VAR])    # Convert to pandas Series\n",
    "\n",
    "# Proceed with MERF analysis for the current subset\n",
    "print(\"---------- Select predictors for training set ----------\")\n",
    "X = train_set.drop([BMI_VAR, ID_VAR], axis=1)\n",
    "Y = train_set[BMI_VAR].to_numpy()  # Convert Y to numeric array\n",
    "Z = np.ones((train_set.shape[0], 1))  # Create random effects matrix with ones\n",
    "\n",
    "# Check lengths before fitting\n",
    "print(f\"Length of X: {len(X)}, Length of clusters_train: {len(clusters_train)}, Length of Y: {len(Y)}\")\n",
    "assert len(X) == len(clusters_train), \"Length of X does not match length of clusters_train\"\n",
    "assert len(X) == len(Y), \"Length of X does not match length of Y\"\n",
    "print(\"Final columns after drop:\", X.columns.to_list())\n",
    "print(\"X train values:\", train_set[BMI_VAR])\n",
    "\n",
    "print(\"---------- Select predictors for test set ----------\")\n",
    "X_new = test_set.drop([BMI_VAR, ID_VAR], axis=1)\n",
    "X_new = X_new[X.columns]  # Reorder and select columns to match training set\n",
    "X_new = X_new.astype(X.dtypes)  # Ensure data types match\n",
    "Y_new = test_set[BMI_VAR].to_numpy()  # Convert Y to numeric array\n",
    "clusters_new = pd.Series(test_set[ID_VAR])  # Convert to pandas Series\n",
    "Z_new = np.ones((len(X_new), 1))  # Create random effects matrix with ones\n",
    "time_new = test_set[TIME_VAR].astype(float).to_numpy()  # Convert time values to numeric arrayo numeric array\n",
    "\n",
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [0.05, 0.1, 0.15],\n",
    "    'max_iter': [2, 10],\n",
    "    'n_splits': [3, 5, 10] #cross-validation\n",
    "}\n",
    "# Create training features\n",
    "# X = train_set.drop(columns=columns_to_drop, errors='ignore')\n",
    "y = train_set[[BMI_VAR]]\n",
    "y = y[BMI_VAR].to_numpy() # Convert Y to numeric array\n",
    "clusters = train_set[ID_VAR].to_numpy() # Get ID variables\n",
    "z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "results = []  # Initialize a list to store the results of each iteration\n",
    "\n",
    "# Loop through all possible combinations of parameters\n",
    "total_combinations = len(list(itertools.product(*param_grid.values())))  # Calculate total combinations\n",
    "for idx, params in enumerate(itertools.product(*param_grid.values())):\n",
    "    n_estimators, max_depth, min_samples_split, max_iter, n_splits = params\n",
    "    \n",
    "    # Print progress\n",
    "    progress_percentage = (idx + 1) / total_combinations * 100\n",
    "    print(f\"Combination: {params}\\n\")\n",
    "    print(f\"Progress: {progress_percentage:.2f}% completed\\n\")\n",
    "    scores, prev, ptev, oob_scores = [], [], [], []  # Initialize lists for scores\n",
    "\n",
    "    # K-fold cross-validation with variable n_splits\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #print(\"Train indices:\", train_index)\n",
    "        #print(\"Test indices:\", test_index)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # Use .iloc for row selection\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clusters_train, clusters_test = clusters[train_index], pd.Series(clusters[test_index])\n",
    "        z_train, z_test = z[train_index], z[test_index]\n",
    "\n",
    "        print(\"Length of clusters_train:\", len(clusters_train))\n",
    "        print(\"Length of clusters_test:\", len(clusters_test))\n",
    "        #print(\"Train indices:\", train_index)\n",
    "        #print(\"Test indices:\", test_index)\n",
    "\n",
    "        model = MERF(\n",
    "                # Specify the fixed effects model as a Random Forest Regressor\n",
    "            fixed_effects_model=RandomForestRegressor(\n",
    "                n_estimators=n_estimators,  # Number of trees in the forest\n",
    "                max_depth=max_depth,  # Maximum depth of each tree\n",
    "                min_samples_split=min_samples_split,  # Minimum samples required to split an internal node\n",
    "                n_jobs=1,  # Number of jobs to run in parallel\n",
    "                oob_score=True  # Whether to use out-of-bag samples to estimate the R^2 on unseen data\n",
    "                ),\n",
    "                # Generalized Linear Model (GLM) early stopping threshold\n",
    "            gll_early_stop_threshold=None,  # No early stopping threshold set\n",
    "                # Maximum number of iterations for the MERF algorithm\n",
    "            max_iterations=max_iter  # Maximum number of iterations to run the MERF algorithm\n",
    "            )\n",
    "        model.fit(X_train.select_dtypes(include=[np.number]), z_train, pd.Series(clusters_train), y_train)\n",
    "        y_pred = model.predict(X_test, z_test, clusters_test)\n",
    "        scores.append(np.mean((y_pred - y_test) ** 2)) # MSE\n",
    "        \n",
    "        # Calculate ptev and prev\n",
    "        total_variance = np.var(y_test)\n",
    "        random_effect_variance = np.var(y_test - y_pred)\n",
    "        fixed_effect_variance = total_variance - random_effect_variance\n",
    "\n",
    "        ptev.append(np.mean(fixed_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "        prev.append(np.mean(random_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "\n",
    "        # Calculate OOB score\n",
    "        forest = model.trained_fe_model\n",
    "        oob_score = round(forest.oob_score_ * 100, 1)  # percent variation\n",
    "        oob_scores.append(oob_score)\n",
    "\n",
    "        # Print ptev, prev, and OOB score for the current iteration\n",
    "        print(f\"Combination, ptev: {np.mean(ptev):.4f}, prev: {np.mean(prev):.4f}, OOB Score: {oob_score:.4f}\")\n",
    "\n",
    "    # Calculate the mean of the scores for the current combination of parameters\n",
    "    mean_score = np.mean(scores)\n",
    "    if mean_score < best_score:\n",
    "        best_score = mean_score\n",
    "        best_params = params\n",
    "\n",
    "    # Append the results of the current iteration to the results list\n",
    "    result_dict = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'max_iter': max_iter,\n",
    "        'n_splits': n_splits,\n",
    "        'mean_mse_score': mean_score,\n",
    "        'mean_prev': np.mean(prev),\n",
    "        'mean_ptev': np.mean(ptev),\n",
    "        'oob_score': np.mean(oob_scores)\n",
    "    }\n",
    "    results.append(result_dict)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "\n",
    "# Convert the results list to a DataFrame and save it to a CSV file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(OUT, index=False)\n",
    "\n",
    "print(\"---------- Run MERF models ----------\")\n",
    "r2_values, results_df = run_merf_analysis2(\n",
    "    X, Y, Z, clusters,\n",
    "    X_new, Y_new, Z_new, clusters_new,\n",
    "    results_df,\n",
    "    output_dir, r2_out, r2_adj_out, \n",
    "    feature_imp_out, results_filename, time_new)\n",
    "\n",
    "# Print the R-squared values and the results DataFrame\n",
    "print(\"R-squared values:\", r2_values)\n",
    "print(\"Results DataFrame:\\n\", results_df)\n",
    "print(\"---------- Done saving Merf output ----------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
