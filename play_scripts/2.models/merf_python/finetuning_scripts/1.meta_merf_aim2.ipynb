{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from merf import MERF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "sns.set_context(\"poster\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (11,8)\n",
    "from merf.merf import MERF\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from merf.viz import plot_merf_training_stats\n",
    "\n",
    "current_dir = os.getcwd() # Get the current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from em_utils import * # import the utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_plots\"\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"---------- Read metadata ----------\")\n",
    "m1_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/clinical/transformed/aim2\"\n",
    "test = read_data(m1_dir, \"a2_test_samples_standard_clinical_feb20.csv\")\n",
    "train = read_data(m1_dir, \"a2_train_samples_standard_clinical_feb20.csv\")\n",
    "full = read_data(m1_dir, \"a2_meta_Transformed_standard_clinical_feb20.csv\")\n",
    "full_raw = read_data(m1_dir, \"a2_meta_not_Transformed_standard_clinical_feb20.csv\")\n",
    "\n",
    "print(full_raw.columns.to_list() == train.columns.to_list())\n",
    "print(train.columns.to_list() == test.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make long format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each meta dataset\n",
    "print(\"---------- Convert metadata to long format ----------\")\n",
    "full_long = make_long(full_raw)\n",
    "full_long['x_t'] = full_long['subject_id'].astype(str) + '.' + full_long['time'].astype(str)\n",
    "\n",
    "train_long = make_long(train)\n",
    "train_long['x_t'] = train_long['subject_id'].astype(str) + '.' + train_long['time'].astype(str)\n",
    "\n",
    "test_long = make_long(test)\n",
    "test_long['x_t'] = test_long['subject_id'].astype(str) + '.' + test_long['time'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_long' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain data outcome_BMI_fnl values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtrain_long\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutcome_BMI_fnl\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull columns after transformation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, full_long\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_list())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain columns after transformation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_long\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_list())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_long' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"train data outcome_BMI_fnl values:\", train_long['outcome_BMI_fnl'])\n",
    "print(\"Full columns after transformation:\", full_long.columns.to_list())\n",
    "print(\"Train columns after transformation:\", train_long.columns.to_list())\n",
    "print(\"Test columns after transformation:\", test_long.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns only if they exist in the DataFrame (since some may not be present after merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'cohort_number', 'record_id', 'x_t']\n",
    "full_long = full_long.drop([col for col in columns_to_drop if col in full.columns], axis=1)\n",
    "train_long = train_long.drop([col for col in columns_to_drop if col in train_long.columns], axis=1)\n",
    "test_long = test_long.drop([col for col in columns_to_drop if col in test_long.columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final columns\n",
    "print(\"Final columns after drop:\", full_long.columns.to_list())\n",
    "print(\"Final test columns after drop:\", test_long.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA \n",
    "test_long = test_long.dropna()\n",
    "train_long = train_long.dropna()\n",
    "full_long = full_long.dropna()\n",
    "raw_train = full_long[full_long['subject_id'].isin(train_long['subject_id'])]\n",
    "raw_test = full_long[full_long['subject_id'].isin(test_long['subject_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- raw train and tax ----------\")\n",
    "print(\"raw_train shape = \", raw_train.shape)\n",
    "print(\"raw_test shape = \", raw_test.shape)\n",
    "print(\"---------- preprocessed train and tax ----------\")\n",
    "print(\"test_long shape = \", test_long.shape)\n",
    "print(\"train_long shape = \", train_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- Select predictors for Basic Raw training set ----------\")\n",
    "train_set = raw_train\n",
    "X = train_set.drop(['outcome_BMI_fnl', 'subject_id'], axis=1)\n",
    "#X = X.drop(columns=['Unnamed: 0_tax', 'x_t'], errors='ignore')\n",
    "Y = train_set[['outcome_BMI_fnl']]\n",
    "Y = Y['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array\n",
    "clusters_train = train_set['subject_id'].to_numpy() # Get ID variables\n",
    "Z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "time = train_set['time'].astype(float).to_numpy() # Get time values as numeric array \n",
    "\n",
    "# Check the final columns\n",
    "print(\"Final columns after drop:\", X.columns.to_list())\n",
    "print(\"X train values:\", train_set['outcome_BMI_fnl'])\n",
    "\n",
    "print(\"---------- Select predictors for Basic Raw test set ----------\")\n",
    "test_set = raw_test\n",
    "X_new = test_set.drop(['outcome_BMI_fnl', 'subject_id'], axis=1)\n",
    "X_new = X_new[X.columns]  # Reorder and select columns to match training set\n",
    "X_new = X_new.astype(X.dtypes)  # Ensure data types match\n",
    "\n",
    "Y_new = test_set['outcome_BMI_fnl'].to_numpy()  # Convert Y to numeric array\n",
    "clusters_new = pd.Series(test_set['subject_id'])  # Convert to pandas Series\n",
    "# Create random effects matrix with ones\n",
    "Z_new = np.ones((len(X_new), 1))\n",
    "time_new = test_set['time'].astype(float).to_numpy()  # Convert time values to numeric array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- RUN MERF preprocessed with participant RE ðŸŒ±ðŸŒ¸ ----------\")\n",
    " # Mixed Effects Random Forest Training with participant RE and time cluster \n",
    "train_set = train_long \n",
    "X_train = train_set.drop(['outcome_BMI_fnl', 'subject_id', 'time'], axis=1).to_numpy()\n",
    "Z_train = np.array((np.ones(len(train_set)), train_set['subject_id'].apply(lambda s: int(s[-3:])))).T\n",
    "clusters_train = pd.Series(train_set['subject_id'].apply(lambda s: int(s[-3:]))).astype(float)  # Convert to float if necessary\n",
    "y_train = train_set[['outcome_BMI_fnl']]\n",
    "y_train = y_train['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dimensions of X_train: {X_train.shape}\")\n",
    "print(f\"Dimensions of Z_train: {Z_train.shape}\")\n",
    "print(f\"Number of unique inputs for clusters_train: {clusters_train.nunique()}\")\n",
    "print(f\"Inputs to clusters_train: {clusters_train}\")\n",
    "print(f\"Dimensions of y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Mixed Effects Random Forests (MERF), the Generalized Log-Likelihood (GLL) is used to evaluate the quality of the model at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MERF if RE on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data (repeat similar steps)\n",
    "test_set = test_long\n",
    "X_test = test_set.drop(['outcome_BMI_fnl', 'subject_id', 'time'], axis=1).to_numpy()\n",
    "Z_test = np.array((np.ones(len(test_set)), test_set['subject_id'].apply(lambda s: int(s[-3:])))).T\n",
    "clusters_test = pd.Series(test_set['subject_id'].apply(lambda s: int(s[-3:]))).astype(float)  # Convert to float if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning wth PREV and PTEV.\n",
    "ptev is calculated as the ratio of fixed effect variance to total variance, while prev is calculated as the ratio of random effect variance to total variance.\n",
    "\n",
    "A higher PTEV is often preferable because it suggests that your model explains a larger portion of the observed variability. However, excessively high PTEV could signal overfitting, where the model memorizes rather than generalizes.\n",
    "\n",
    "The optimal PREV depends on your data and goals. If your model is designed to capture group- or subject-level variability, a higher PREV might be desirable. However, in most cases, excessively high PREV might indicate the model isn't capturing key fixed effects, potentially leading to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- MERF with finetuning RE ðŸŒ± ----------\")\n",
    "import itertools \n",
    "import pandas as pd\n",
    "\n",
    "print(\"---------- Select predictors for Basic Raw training set ----------\")\n",
    "train_set = raw_train\n",
    "X = train_set.drop(['outcome_BMI_fnl', 'subject_id', 'x_t'], axis=1)\n",
    "print(f\"X dimensions: {X.shape}\")\n",
    "y = train_set[['outcome_BMI_fnl']]\n",
    "y = y['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array\n",
    "print(f\"y dimensions: {y.shape}\")\n",
    "clusters = train_set['subject_id'].to_numpy() # Get ID variables\n",
    "print(f\"clusters dimensions: {clusters.shape}\")\n",
    "z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "print(f\"z dimensions: {z.shape}\")\n",
    "\n",
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [0.05, 0.1, 0.15],\n",
    "    'max_iter': [2, 10],\n",
    "    'n_splits': [3, 5, 10]  # Added n_splits for cross-validation\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Initialize a list to store the results of each iteration\n",
    "results = []\n",
    "\n",
    "# Loop through all possible combinations of parameters\n",
    "for params in itertools.product(*param_grid.values()):\n",
    "        n_estimators, max_depth, min_samples_split, max_iter, n_splits = params\n",
    "        print(f\"Combination: {params}\\n\")\n",
    "        scores = []\n",
    "        prev = []\n",
    "        ptev = []\n",
    "        oob_scores = []  # Initialize a list to store OOB scores\n",
    "\n",
    "        # K-fold cross-validation with variable n_splits\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # Use .iloc for row selection\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clusters_train, clusters_test = clusters[train_index], pd.Series(clusters[test_index])\n",
    "            z_train, z_test = z[train_index], z[test_index]\n",
    "            model = MERF(\n",
    "                # Specify the fixed effects model as a Random Forest Regressor\n",
    "                fixed_effects_model=RandomForestRegressor(\n",
    "                    n_estimators=n_estimators,  # Number of trees in the forest\n",
    "                    max_depth=max_depth,  # Maximum depth of each tree\n",
    "                    min_samples_split=min_samples_split,  # Minimum samples required to split an internal node\n",
    "                    n_jobs=1,  # Number of jobs to run in parallel\n",
    "                    oob_score=True  # Whether to use out-of-bag samples to estimate the R^2 on unseen data\n",
    "                ),\n",
    "                # Generalized Linear Model (GLM) early stopping threshold\n",
    "                gll_early_stop_threshold=None,  # No early stopping threshold set\n",
    "                # Maximum number of iterations for the MERF algorithm\n",
    "                max_iterations=max_iter  # Maximum number of iterations to run the MERF algorithm\n",
    "            )\n",
    "            model.fit(X_train.select_dtypes(include=[np.number]), z_train, pd.Series(clusters_train), y_train)\n",
    "            y_pred = model.predict(X_test, z_test, clusters_test)\n",
    "            scores.append(np.mean((y_pred - y_test) ** 2)) # MSE\n",
    "            \n",
    "            # Calculate ptev and prev\n",
    "            total_variance = np.var(y_test) #calculates the total variance of the predicted values\n",
    "            random_effect_variance = np.var(y_test - y_pred)  # Variance of residuals\n",
    "            fixed_effect_variance = total_variance - random_effect_variance\n",
    "\n",
    "            ptev.append(np.mean(fixed_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "            prev.append(np.mean(random_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "\n",
    "            # Calculate OOB score\n",
    "            forest = model.trained_fe_model\n",
    "            oob_score = round(forest.oob_score_*100, 1)  # percent variation\n",
    "            oob_scores.append(oob_score)  # Append OOB score to the list\n",
    "\n",
    "            # Print ptev, prev, and OOB score for the current iteration\n",
    "            print(f\"Combination, ptev: {np.mean(ptev):.4f}, prev: {np.mean(prev):.4f}, OOB Score: {oob_score:.4f}\")\n",
    "\n",
    "        # Calculate the mean of the scores for the current combination of parameters\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_prev = np.mean(prev)\n",
    "        mean_ptev = np.mean(ptev)\n",
    "        mean_oob_score = np.mean(oob_scores)  # Calculate the mean of OOB scores\n",
    "        if mean_score < best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "\n",
    "        # Append the results of the current iteration to the results list\n",
    "        # Create a result dictionary with individual scores and mean scores\n",
    "        result_dict = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'max_iter': max_iter,\n",
    "            'n_splits': n_splits,  # Added n_splits to the result dictionary\n",
    "            'mse_score_1': scores[0] if len(scores) > 0 else None,\n",
    "            'mse_score_2': scores[1] if len(scores) > 1 else None,\n",
    "            'mse_score_3': scores[2] if len(scores) > 2 else None,\n",
    "            'mse_score_4': scores[3] if len(scores) > 3 else None,\n",
    "            'mse_score_5': scores[4] if len(scores) > 4 else None,\n",
    "            'mse_score_6': scores[5] if len(scores) > 5 else None,\n",
    "            'mse_score_7': scores[6] if len(scores) > 6 else None,\n",
    "            'mse_score_8': scores[7] if len(scores) > 7 else None,\n",
    "            'mse_score_9': scores[8] if len(scores) > 8 else None,\n",
    "            'mse_score_10': scores[9] if len(scores) > 9 else None,\n",
    "            'mean_mse_score': mean_score,\n",
    "            'prev_1': prev[0] if len(prev) > 0 else None,\n",
    "            'prev_2': prev[1] if len(prev) > 1 else None,\n",
    "            'prev_3': prev[2] if len(prev) > 2 else None,\n",
    "            'prev_4': prev[3] if len(prev) > 3 else None,\n",
    "            'prev_5': prev[4] if len(prev) > 4 else None,\n",
    "            'prev_6': prev[5] if len(prev) > 5 else None,\n",
    "            'prev_7': prev[6] if len(prev) > 6 else None,\n",
    "            'prev_8': prev[7] if len(prev) > 7 else None,\n",
    "            'prev_9': prev[8] if len(prev) > 8 else None,\n",
    "            'prev_10': prev[9] if len(prev) > 9 else None,\n",
    "            'mean_prev': mean_prev,\n",
    "            'ptev_1': ptev[0] if len(ptev) > 0 else None,\n",
    "            'ptev_2': ptev[1] if len(ptev) > 1 else None,\n",
    "            'ptev_3': ptev[2] if len(ptev) > 2 else None,\n",
    "            'ptev_4': ptev[3] if len(ptev) > 3 else None,\n",
    "            'ptev_5': ptev[4] if len(ptev) > 4 else None,\n",
    "            'ptev_6': ptev[5] if len(ptev) > 5 else None,\n",
    "            'ptev_7': ptev[6] if len(ptev) > 6 else None,\n",
    "            'ptev_8': ptev[7] if len(ptev) > 7 else None,\n",
    "            'ptev_9': ptev[8] if len(ptev) > 8 else None,\n",
    "            'ptev_10': ptev[9] if len(ptev) > 9 else None,\n",
    "            'mean_ptev': mean_ptev,\n",
    "            'oob_1': oob_scores[0] if len(oob_scores) > 0 else None,\n",
    "            'oob_2': oob_scores[1] if len(oob_scores) > 1 else None,\n",
    "            'oob_3': oob_scores[2] if len(oob_scores) > 2 else None,\n",
    "            'oob_4': oob_scores[3] if len(oob_scores) > 3 else None,\n",
    "            'oob_5': oob_scores[4] if len(oob_scores) > 4 else None,\n",
    "            'oob_6': oob_scores[5] if len(oob_scores) > 5 else None,\n",
    "            'oob_7': oob_scores[6] if len(oob_scores) > 6 else None,\n",
    "            'oob_8': oob_scores[7] if len(oob_scores) > 7 else None,\n",
    "            'oob_9': oob_scores[8] if len(oob_scores) > 8 else None,\n",
    "            'oob_10': oob_scores[9] if len(oob_scores) > 9 else None,\n",
    "            'oob_score': mean_oob_score\n",
    "        }\n",
    "        # Append the result dictionary to the results list\n",
    "        results.append(result_dict)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "\n",
    "# Convert the results list to a DataFrame and save it to a CSV file\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/1.clinical\"\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'{df_dir}/dec5_maggie_params_tuning_raw_clinical_oob.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
