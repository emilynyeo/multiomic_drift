{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from merf import MERF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "sns.set_context(\"poster\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (11,8)\n",
    "from merf.merf import MERF\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from merf.viz import plot_merf_training_stats\n",
    "\n",
    "current_dir = os.getcwd() # Get the current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from em_utils import * # import the utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_plots\"\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"---------- Read metadata ----------\")\n",
    "m1_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/clinical/transformed/aim2\"\n",
    "test = read_data(m1_dir, \"a2_test_samples_standard_clinical_feb20.csv\")\n",
    "train = read_data(m1_dir, \"a2_train_samples_standard_clinical_feb20.csv\")\n",
    "full = read_data(m1_dir, \"a2_meta_Transformed_standard_clinical_feb20.csv\")\n",
    "full_raw = read_data(m1_dir, \"a2_meta_not_Transformed_standard_clinical_feb20.csv\")\n",
    "\n",
    "print(full_raw.columns.to_list() == train.columns.to_list())\n",
    "print(train.columns.to_list() == test.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of train_long: (97, 29)\n",
      "Dimensions of test_long: (24, 29)\n",
      "Dimensions of full_long: (121, 29)\n"
     ]
    }
   ],
   "source": [
    "# Print dimensions of each DataFrame\n",
    "print(\"Dimensions of train_long:\", train.shape)\n",
    "print(\"Dimensions of test_long:\", test.shape)\n",
    "print(\"Dimensions of full_long:\", full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns : ['Unnamed: 0', 'record_id', 'subject_id', 'randomized_group', 'cohort_number', 'sex', 'race', 'age', 'outcome_BMI_fnl_BL', 'Glucose_BL', 'HOMA_IR_BL', 'Insulin_endo_BL', 'HDL_Total_Direct_lipid_BL', 'LDL_Calculated_BL', 'Triglyceride_lipid_BL', 'outcome_BMI_fnl_6m', 'Glucose_6m', 'HOMA_IR_6m', 'Insulin_endo_6m', 'HDL_Total_Direct_lipid_6m', 'LDL_Calculated_6m', 'Triglyceride_lipid_6m', 'outcome_BMI_fnl_12m', 'Glucose_12m', 'HOMA_IR_12m', 'Insulin_endo_12m', 'HDL_Total_Direct_lipid_12m', 'LDL_Calculated_12m', 'Triglyceride_lipid_12m']\n",
      "Test columns: ['Unnamed: 0', 'record_id', 'subject_id', 'randomized_group', 'cohort_number', 'sex', 'race', 'age', 'outcome_BMI_fnl_BL', 'Glucose_BL', 'HOMA_IR_BL', 'Insulin_endo_BL', 'HDL_Total_Direct_lipid_BL', 'LDL_Calculated_BL', 'Triglyceride_lipid_BL', 'outcome_BMI_fnl_6m', 'Glucose_6m', 'HOMA_IR_6m', 'Insulin_endo_6m', 'HDL_Total_Direct_lipid_6m', 'LDL_Calculated_6m', 'Triglyceride_lipid_6m', 'outcome_BMI_fnl_12m', 'Glucose_12m', 'HOMA_IR_12m', 'Insulin_endo_12m', 'HDL_Total_Direct_lipid_12m', 'LDL_Calculated_12m', 'Triglyceride_lipid_12m']\n"
     ]
    }
   ],
   "source": [
    "print(\"Train columns :\", train.columns.to_list())\n",
    "print(\"Test columns:\", test.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns : ['Unnamed: 0', 'record_id', 'subject_id', 'randomized_group', 'cohort_number', 'sex', 'race', 'age', 'outcome_BMI_fnl_BL', 'Glucose_BL', 'HOMA_IR_BL', 'Insulin_endo_BL', 'HDL_Total_Direct_lipid_BL', 'LDL_Calculated_BL', 'Triglyceride_lipid_BL', 'outcome_BMI_fnl_6m', 'Glucose_6m', 'HOMA_IR_6m', 'Insulin_endo_6m', 'HDL_Total_Direct_lipid_6m', 'LDL_Calculated_6m', 'Triglyceride_lipid_6m', 'outcome_BMI_fnl_12m', 'Glucose_12m', 'HOMA_IR_12m', 'Insulin_endo_12m', 'HDL_Total_Direct_lipid_12m', 'LDL_Calculated_12m', 'Triglyceride_lipid_12m']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (97,) (8,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain columns :\u001b[39m\u001b[38;5;124m\"\u001b[39m, train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mto_list())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m    long_format \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmelt(train, id_vars\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubject_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moriginal_columns\u001b[49m, value_vars\u001b[38;5;241m=\u001b[39mvalue_vars, \n\u001b[1;32m      8\u001b[0m                              var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m'\u001b[39m, value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeyError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/researchVenv/lib/python3.13/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/researchVenv/lib/python3.13/site-packages/pandas/core/arraylike.py:186\u001b[0m, in \u001b[0;36mOpsMixin.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__add__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    Get Addition of DataFrame and other, column-wise.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    moose     3.0     NaN\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/researchVenv/lib/python3.13/site-packages/pandas/core/series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/researchVenv/lib/python3.13/site-packages/pandas/core/base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/researchVenv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m~/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/researchVenv/lib/python3.13/site-packages/pandas/core/ops/array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    215\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(expressions\u001b[38;5;241m.\u001b[39mevaluate, op)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    222\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/researchVenv/lib/python3.13/site-packages/pandas/core/computation/expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[0;32m~/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/researchVenv/lib/python3.13/site-packages/pandas/core/computation/expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TEST_MODE:\n\u001b[1;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (97,) (8,) "
     ]
    }
   ],
   "source": [
    "# Step 1: Identify columns to melt\n",
    "value_vars = [col for col in train.columns if col.endswith(('_BL', '_6m', '_12m'))]\n",
    "# Identify the original columns to keep\n",
    "original_columns = [col for col in train.columns if col not in value_vars]\n",
    "print(\"Train columns :\", train.columns.to_list())\n",
    "try:\n",
    "   long_format = pd.melt(train, id_vars=train['subject_id'] + original_columns, value_vars=value_vars, \n",
    "                             var_name='variable', value_name='value')\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")\n",
    "    print(\"Available columns:\", train.columns.tolist())\n",
    "\n",
    "# Step 2: Melt the DataFrame\n",
    "long_format = pd.melt(train, id_vars=train['subject_id'] + original_columns, value_vars=value_vars, \n",
    "                      var_name='variable', value_name='value')\n",
    "\n",
    "# Step 3: Extract time information and prefix\n",
    "long_format['time'] = long_format['variable'].str.extract(r'_(BL|6m|12m)')[0]\n",
    "long_format['time'] = long_format['time'].replace({'BL': 0, '6m': 6, '12m': 12})\n",
    "\n",
    "# Step 4: Extract the prefix (remove the suffix)\n",
    "long_format['variable'] = long_format['variable'].str.replace(r'_(BL|6m|12m)', '', regex=True)\n",
    "\n",
    "# Step 5: Pivot the DataFrame to get the desired format\n",
    "final_long_format = long_format.pivot_table(index=['subject_id', 'time'] + original_columns, \n",
    "                                             columns='variable', \n",
    "                                             values='value').reset_index()\n",
    "\n",
    "# Optional: Rename the columns to flatten the MultiIndex\n",
    "final_long_format.columns.name = None  # Remove the name of the columns\n",
    "final_long_format = final_long_format.rename_axis(None, axis=1)  # Remove the name of the index\n",
    "\n",
    "# Display the final long format DataFrame\n",
    "print(final_long_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    subject_id  time    Glucose  HDL_Total_Direct_lipid   HOMA_IR  \\\n",
      "0      ACO-053     0  11.565222                6.636147  1.299463   \n",
      "1      ACO-053     6  10.663916                6.611455  1.193159   \n",
      "2      ACO-053    12  10.712033                7.619975  0.603652   \n",
      "3      ADA-105     0  10.076629                4.136819  0.990680   \n",
      "4      ADA-105     6  12.013779                4.166122  1.351743   \n",
      "..         ...   ...        ...                     ...       ...   \n",
      "286    YOR-103     6  12.013779                4.166122  1.351743   \n",
      "287    YOR-103    12  10.952753                4.498539  0.540065   \n",
      "288    YSU-097     0  11.794237                3.705900  1.325195   \n",
      "289    YSU-097     6  12.958683                4.166122  1.242784   \n",
      "290    YSU-097    12  12.517432                3.672277  0.617218   \n",
      "\n",
      "     Insulin_endo  LDL_Calculated  Triglyceride_lipid  outcome_BMI_fnl  \n",
      "0        1.311064        3.100593            1.467331         7.907706  \n",
      "1        1.509470        3.320281            1.551250         7.250215  \n",
      "2        1.329815        3.293866            1.388248         6.794972  \n",
      "3        1.147181        3.767387            1.817933         8.211329  \n",
      "4        1.509470        3.779469            1.595826         6.992553  \n",
      "..            ...             ...                 ...              ...  \n",
      "286      1.509470        3.779469            1.595826         5.610857  \n",
      "287      1.163588        3.419347            1.581957         5.423882  \n",
      "288      1.311064        2.067062            1.051804         6.986171  \n",
      "289      1.293832        3.284959            1.283793         7.042063  \n",
      "290      1.163588        2.791944            1.452818         6.513035  \n",
      "\n",
      "[291 rows x 9 columns]\n",
      "Dimensions of full_long: (291, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/kqxc4rwn4lj10fzv9hw4xxsr0000gn/T/ipykernel_51513/1534173390.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  long_format['time'] = long_format['time'].replace({'BL': 0, '6m': 6, '12m': 12})\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify columns to melt\n",
    "value_vars = [col for col in train.columns if col.endswith(('_BL', '_6m', '_12m'))]\n",
    "\n",
    "# Step 2: Melt the DataFrame\n",
    "long_format = pd.melt(train, id_vars=['subject_id'], value_vars=value_vars, \n",
    "                      var_name='variable', value_name='value')\n",
    "\n",
    "# Step 3: Extract time information and prefix\n",
    "long_format['time'] = long_format['variable'].str.extract(r'_(BL|6m|12m)')[0]\n",
    "long_format['time'] = long_format['time'].replace({'BL': 0, '6m': 6, '12m': 12})\n",
    "\n",
    "# Step 4: Extract the prefix (remove the suffix)\n",
    "long_format['variable'] = long_format['variable'].str.replace(r'_(BL|6m|12m)', '', regex=True)\n",
    "\n",
    "# Step 5: Pivot the DataFrame to get the desired format\n",
    "final_long_format = long_format.pivot_table(index=['subject_id', 'time'], \n",
    "                                             columns='variable', \n",
    "                                             values='value').reset_index()\n",
    "\n",
    "# Optional: Rename the columns to flatten the MultiIndex\n",
    "final_long_format.columns.name = None  # Remove the name of the columns\n",
    "final_long_format = final_long_format.rename_axis(None, axis=1)  # Remove the name of the index\n",
    "\n",
    "# Display the final long format DataFrame\n",
    "print(final_long_format)\n",
    "print(\"Dimensions of full_long:\", final_long_format.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     subject_id            variable     value  time\n",
      "0       LFI-003     outcome_BMI_fnl  7.833650     0\n",
      "1       ROL-006     outcome_BMI_fnl  8.042152     0\n",
      "2       AKE-009     outcome_BMI_fnl  6.549259     0\n",
      "3       HGI-010     outcome_BMI_fnl  6.708535     0\n",
      "4       AKI-011     outcome_BMI_fnl  7.772516     0\n",
      "...         ...                 ...       ...   ...\n",
      "2032    SKA-195  Triglyceride_lipid  1.565815    12\n",
      "2033    KHU-196  Triglyceride_lipid  1.275251    12\n",
      "2034    LPF-198  Triglyceride_lipid  1.307536    12\n",
      "2035    KEL-199  Triglyceride_lipid  1.985518    12\n",
      "2036    BIN-201  Triglyceride_lipid  1.275251    12\n",
      "\n",
      "[2037 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/kqxc4rwn4lj10fzv9hw4xxsr0000gn/T/ipykernel_51513/1373130548.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  long_format_train['time'] = long_format_train['time'].replace({'BL': 0, '6m': 6, '12m': 12})\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify columns to melt\n",
    "value_vars = [col for col in train.columns if col.endswith(('_BL', '_6m', '_12m'))]\n",
    "\n",
    "# Step 2: Melt the DataFrame\n",
    "long_format_train = pd.melt(train, id_vars=['subject_id'], value_vars=value_vars, \n",
    "                      var_name='variable', value_name='value')\n",
    "\n",
    "# Step 3: Extract time information and prefix\n",
    "long_format_train['time'] = long_format_train['variable'].str.extract(r'_(BL|6m|12m)')[0]\n",
    "long_format_train['time'] = long_format_train['time'].replace({'BL': 0, '6m': 6, '12m': 12})\n",
    "\n",
    "# Step 4: Extract the prefix (remove the suffix)\n",
    "long_format_train['variable'] = long_format_train['variable'].str.replace(r'_(BL|6m|12m)', '', regex=True)\n",
    "\n",
    "# Optional: Drop the original 'variable' column if not needed\n",
    "# long_format = long_format.drop(columns=['variable'])\n",
    "\n",
    "# Display the long format DataFrame\n",
    "print(long_format_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject IDs in train_long: ['LFI-003', 'ROL-006', 'AKE-009', 'HGI-010', 'AKI-011', 'CSH-012', 'ASO-013', 'LBU-015', 'TFA-016', 'LVA-017', 'SBO-020', 'NTA-021', 'RHP-023', 'MST-025', 'CED-026', 'JFU-027', 'SSH-028', 'KGI-029', 'JUT-032', 'KCE-034', 'VCA-041', 'SCA-043', 'EKR-045', 'LBL-047', 'NBI-048', 'CLE-049', 'AWA-052', 'ACO-053', 'AHE-055', 'TSL-056', 'CAM-057', 'SGA-062', 'NDI-067', 'MES-068', 'NBI-069', 'AGA-071', 'KRI-072', 'MGA-076', 'SEG-080', 'AWA-083', 'KBU-085', 'TDU-086', 'MCA-088', 'MSH-091', 'EJO-092', 'YSU-097', 'YOR-103', 'BKN-104', 'ADA-105', 'JCA-109', 'LMC-111', 'MWE-112', 'TRO-113', 'RAE-114', 'TBU-115', 'JUG-116', 'MHO-117', 'MFB-118', 'MAR-119', 'KWA-122', 'JKN-127', 'AME-128', 'ATA-129', 'EBE-130', 'RLA-132', 'MWO-133', 'EKA-135', 'AKO-139', 'KWA-141', 'TSH-146', 'LEL-147', 'LDO-148', 'NPO-149', 'SDA-150', 'MWY-152', 'BMI-156', 'KBR-162', 'ALO-163', 'BMO-164', 'QNG-166', 'KHE-170', 'NCO-171', 'TFA-172', 'BSA-174', 'MBA-176', 'EHI-177', 'SLO-178', 'EPO-182', 'EVO-184', 'GFU-188', 'CCO-189', 'BAN-193', 'SKA-195', 'KHU-196', 'LPF-198', 'KEL-199', 'BIN-201']\n",
      "Subject IDs in test_long: ['CBO-004', 'WPE-005', 'BHO-014', 'JPO-022', 'KPA-042', 'MCW-065', 'COW-066', 'CEL-073', 'CIS-077', 'JJO-093', 'ZVU-096', 'FWI-098', 'LJA-101', 'MLU-106', 'MDI-107', 'JVE-126', 'JDI-140', 'LZD-142', 'MWE-143', 'AAL-144', 'AME-157', 'CWA-161', 'AWI-167', 'MBA-187']\n",
      "Subject IDs in full_long: ['LFI-003', 'CBO-004', 'WPE-005', 'ROL-006', 'AKE-009', 'HGI-010', 'AKI-011', 'CSH-012', 'ASO-013', 'BHO-014', 'LBU-015', 'TFA-016', 'LVA-017', 'SBO-020', 'NTA-021', 'JPO-022', 'RHP-023', 'MST-025', 'CED-026', 'JFU-027', 'SSH-028', 'KGI-029', 'JUT-032', 'KCE-034', 'VCA-041', 'KPA-042', 'SCA-043', 'EKR-045', 'LBL-047', 'NBI-048', 'CLE-049', 'AWA-052', 'ACO-053', 'AHE-055', 'TSL-056', 'CAM-057', 'SGA-062', 'MCW-065', 'COW-066', 'NDI-067', 'MES-068', 'NBI-069', 'AGA-071', 'KRI-072', 'CEL-073', 'MGA-076', 'CIS-077', 'SEG-080', 'AWA-083', 'KBU-085', 'TDU-086', 'MCA-088', 'MSH-091', 'EJO-092', 'JJO-093', 'ZVU-096', 'YSU-097', 'FWI-098', 'LJA-101', 'YOR-103', 'BKN-104', 'ADA-105', 'MLU-106', 'MDI-107', 'JCA-109', 'LMC-111', 'MWE-112', 'TRO-113', 'RAE-114', 'TBU-115', 'JUG-116', 'MHO-117', 'MFB-118', 'MAR-119', 'KWA-122', 'JVE-126', 'JKN-127', 'AME-128', 'ATA-129', 'EBE-130', 'RLA-132', 'MWO-133', 'EKA-135', 'AKO-139', 'JDI-140', 'KWA-141', 'LZD-142', 'MWE-143', 'AAL-144', 'TSH-146', 'LEL-147', 'LDO-148', 'NPO-149', 'SDA-150', 'MWY-152', 'BMI-156', 'AME-157', 'CWA-161', 'KBR-162', 'ALO-163', 'BMO-164', 'QNG-166', 'AWI-167', 'KHE-170', 'NCO-171', 'TFA-172', 'BSA-174', 'MBA-176', 'EHI-177', 'SLO-178', 'EPO-182', 'EVO-184', 'MBA-187', 'GFU-188', 'CCO-189', 'BAN-193', 'SKA-195', 'KHU-196', 'LPF-198', 'KEL-199', 'BIN-201']\n"
     ]
    }
   ],
   "source": [
    "# Print subject_id values from train_long DataFrame\n",
    "print(\"Subject IDs in train_long:\", train['subject_id'].to_list())\n",
    "\n",
    "# Print subject_id values from test_long DataFrame\n",
    "print(\"Subject IDs in test_long:\", test['subject_id'].to_list())\n",
    "\n",
    "# Print subject_id values from full_long DataFrame\n",
    "print(\"Subject IDs in full_long:\", full['subject_id'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make long format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each meta dataset\n",
    "print(\"---------- Convert metadata to long format ----------\")\n",
    "full_long = make_long(full_raw)\n",
    "full_long['x_t'] = full_long['subject_id'].astype(str) + '.' + full_long['time'].astype(str)\n",
    "\n",
    "train_long = make_long(train)\n",
    "train_long['x_t'] = train_long['subject_id'].astype(str) + '.' + train_long['time'].astype(str)\n",
    "\n",
    "test_long = make_long(test)\n",
    "test_long['x_t'] = test_long['subject_id'].astype(str) + '.' + test_long['time'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train data outcome_BMI_fnl values:\", train_long['outcome_BMI_fnl'])\n",
    "print(\"Full columns after transformation:\", full_long.columns.to_list())\n",
    "print(\"Train columns after transformation:\", train_long.columns.to_list())\n",
    "print(\"Test columns after transformation:\", test_long.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns only if they exist in the DataFrame (since some may not be present after merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0', 'cohort_number', 'record_id', 'x_t']\n",
    "full_long = full_long.drop([col for col in columns_to_drop if col in full.columns], axis=1)\n",
    "train_long = train_long.drop([col for col in columns_to_drop if col in train_long.columns], axis=1)\n",
    "test_long = test_long.drop([col for col in columns_to_drop if col in test_long.columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final columns\n",
    "print(\"Final columns after drop:\", full_long.columns.to_list())\n",
    "print(\"Final test columns after drop:\", test_long.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA \n",
    "test_long = test_long.dropna()\n",
    "train_long = train_long.dropna()\n",
    "full_long = full_long.dropna()\n",
    "raw_train = full_long[full_long['subject_id'].isin(train_long['subject_id'])]\n",
    "raw_test = full_long[full_long['subject_id'].isin(test_long['subject_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- raw train and tax ----------\")\n",
    "print(\"raw_train shape = \", raw_train.shape)\n",
    "print(\"raw_test shape = \", raw_test.shape)\n",
    "print(\"---------- preprocessed train and tax ----------\")\n",
    "print(\"test_long shape = \", test_long.shape)\n",
    "print(\"train_long shape = \", train_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- Select predictors for Basic Raw training set ----------\")\n",
    "train_set = raw_train\n",
    "X = train_set.drop(['outcome_BMI_fnl', 'subject_id'], axis=1)\n",
    "#X = X.drop(columns=['Unnamed: 0_tax', 'x_t'], errors='ignore')\n",
    "Y = train_set[['outcome_BMI_fnl']]\n",
    "Y = Y['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array\n",
    "clusters_train = train_set['subject_id'].to_numpy() # Get ID variables\n",
    "Z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "time = train_set['time'].astype(float).to_numpy() # Get time values as numeric array \n",
    "\n",
    "# Check the final columns\n",
    "print(\"Final columns after drop:\", X.columns.to_list())\n",
    "print(\"X train values:\", train_set['outcome_BMI_fnl'])\n",
    "\n",
    "print(\"---------- Select predictors for Basic Raw test set ----------\")\n",
    "test_set = raw_test\n",
    "X_new = test_set.drop(['outcome_BMI_fnl', 'subject_id'], axis=1)\n",
    "X_new = X_new[X.columns]  # Reorder and select columns to match training set\n",
    "X_new = X_new.astype(X.dtypes)  # Ensure data types match\n",
    "\n",
    "Y_new = test_set['outcome_BMI_fnl'].to_numpy()  # Convert Y to numeric array\n",
    "clusters_new = pd.Series(test_set['subject_id'])  # Convert to pandas Series\n",
    "# Create random effects matrix with ones\n",
    "Z_new = np.ones((len(X_new), 1))\n",
    "time_new = test_set['time'].astype(float).to_numpy()  # Convert time values to numeric array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- RUN MERF preprocessed with participant RE ðŸŒ±ðŸŒ¸ ----------\")\n",
    " # Mixed Effects Random Forest Training with participant RE and time cluster \n",
    "train_set = train_long \n",
    "X_train = train_set.drop(['outcome_BMI_fnl', 'subject_id', 'time'], axis=1).to_numpy()\n",
    "Z_train = np.array((np.ones(len(train_set)), train_set['subject_id'].apply(lambda s: int(s[-3:])))).T\n",
    "clusters_train = pd.Series(train_set['subject_id'].apply(lambda s: int(s[-3:]))).astype(float)  # Convert to float if necessary\n",
    "y_train = train_set[['outcome_BMI_fnl']]\n",
    "y_train = y_train['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dimensions of X_train: {X_train.shape}\")\n",
    "print(f\"Dimensions of Z_train: {Z_train.shape}\")\n",
    "print(f\"Number of unique inputs for clusters_train: {clusters_train.nunique()}\")\n",
    "print(f\"Inputs to clusters_train: {clusters_train}\")\n",
    "print(f\"Dimensions of y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Mixed Effects Random Forests (MERF), the Generalized Log-Likelihood (GLL) is used to evaluate the quality of the model at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MERF if RE on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data (repeat similar steps)\n",
    "test_set = test_long\n",
    "X_test = test_set.drop(['outcome_BMI_fnl', 'subject_id', 'time'], axis=1).to_numpy()\n",
    "Z_test = np.array((np.ones(len(test_set)), test_set['subject_id'].apply(lambda s: int(s[-3:])))).T\n",
    "clusters_test = pd.Series(test_set['subject_id'].apply(lambda s: int(s[-3:]))).astype(float)  # Convert to float if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning wth PREV and PTEV.\n",
    "ptev is calculated as the ratio of fixed effect variance to total variance, while prev is calculated as the ratio of random effect variance to total variance.\n",
    "\n",
    "A higher PTEV is often preferable because it suggests that your model explains a larger portion of the observed variability. However, excessively high PTEV could signal overfitting, where the model memorizes rather than generalizes.\n",
    "\n",
    "The optimal PREV depends on your data and goals. If your model is designed to capture group- or subject-level variability, a higher PREV might be desirable. However, in most cases, excessively high PREV might indicate the model isn't capturing key fixed effects, potentially leading to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- MERF with finetuning RE ðŸŒ± ----------\")\n",
    "import itertools \n",
    "import pandas as pd\n",
    "\n",
    "print(\"---------- Select predictors for Basic Raw training set ----------\")\n",
    "train_set = raw_train\n",
    "X = train_set.drop(['outcome_BMI_fnl', 'subject_id', 'x_t'], axis=1)\n",
    "print(f\"X dimensions: {X.shape}\")\n",
    "y = train_set[['outcome_BMI_fnl']]\n",
    "y = y['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array\n",
    "print(f\"y dimensions: {y.shape}\")\n",
    "clusters = train_set['subject_id'].to_numpy() # Get ID variables\n",
    "print(f\"clusters dimensions: {clusters.shape}\")\n",
    "z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "print(f\"z dimensions: {z.shape}\")\n",
    "\n",
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [0.05, 0.1, 0.15],\n",
    "    'max_iter': [2, 10],\n",
    "    'n_splits': [3, 5, 10]  # Added n_splits for cross-validation\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "# Initialize a list to store the results of each iteration\n",
    "results = []\n",
    "\n",
    "# Loop through all possible combinations of parameters\n",
    "for params in itertools.product(*param_grid.values()):\n",
    "        n_estimators, max_depth, min_samples_split, max_iter, n_splits = params\n",
    "        print(f\"Combination: {params}\\n\")\n",
    "        scores = []\n",
    "        prev = []\n",
    "        ptev = []\n",
    "        oob_scores = []  # Initialize a list to store OOB scores\n",
    "\n",
    "        # K-fold cross-validation with variable n_splits\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # Use .iloc for row selection\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clusters_train, clusters_test = clusters[train_index], pd.Series(clusters[test_index])\n",
    "            z_train, z_test = z[train_index], z[test_index]\n",
    "            model = MERF(\n",
    "                # Specify the fixed effects model as a Random Forest Regressor\n",
    "                fixed_effects_model=RandomForestRegressor(\n",
    "                    n_estimators=n_estimators,  # Number of trees in the forest\n",
    "                    max_depth=max_depth,  # Maximum depth of each tree\n",
    "                    min_samples_split=min_samples_split,  # Minimum samples required to split an internal node\n",
    "                    n_jobs=1,  # Number of jobs to run in parallel\n",
    "                    oob_score=True  # Whether to use out-of-bag samples to estimate the R^2 on unseen data\n",
    "                ),\n",
    "                # Generalized Linear Model (GLM) early stopping threshold\n",
    "                gll_early_stop_threshold=None,  # No early stopping threshold set\n",
    "                # Maximum number of iterations for the MERF algorithm\n",
    "                max_iterations=max_iter  # Maximum number of iterations to run the MERF algorithm\n",
    "            )\n",
    "            model.fit(X_train.select_dtypes(include=[np.number]), z_train, pd.Series(clusters_train), y_train)\n",
    "            y_pred = model.predict(X_test, z_test, clusters_test)\n",
    "            scores.append(np.mean((y_pred - y_test) ** 2)) # MSE\n",
    "            \n",
    "            # Calculate ptev and prev\n",
    "            total_variance = np.var(y_test) #calculates the total variance of the predicted values\n",
    "            random_effect_variance = np.var(y_test - y_pred)  # Variance of residuals\n",
    "            fixed_effect_variance = total_variance - random_effect_variance\n",
    "\n",
    "            ptev.append(np.mean(fixed_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "            prev.append(np.mean(random_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "\n",
    "            # Calculate OOB score\n",
    "            forest = model.trained_fe_model\n",
    "            oob_score = round(forest.oob_score_*100, 1)  # percent variation\n",
    "            oob_scores.append(oob_score)  # Append OOB score to the list\n",
    "\n",
    "            # Print ptev, prev, and OOB score for the current iteration\n",
    "            print(f\"Combination, ptev: {np.mean(ptev):.4f}, prev: {np.mean(prev):.4f}, OOB Score: {oob_score:.4f}\")\n",
    "\n",
    "        # Calculate the mean of the scores for the current combination of parameters\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_prev = np.mean(prev)\n",
    "        mean_ptev = np.mean(ptev)\n",
    "        mean_oob_score = np.mean(oob_scores)  # Calculate the mean of OOB scores\n",
    "        if mean_score < best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "\n",
    "        # Append the results of the current iteration to the results list\n",
    "        # Create a result dictionary with individual scores and mean scores\n",
    "        result_dict = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'max_iter': max_iter,\n",
    "            'n_splits': n_splits,  # Added n_splits to the result dictionary\n",
    "            'mse_score_1': scores[0] if len(scores) > 0 else None,\n",
    "            'mse_score_2': scores[1] if len(scores) > 1 else None,\n",
    "            'mse_score_3': scores[2] if len(scores) > 2 else None,\n",
    "            'mse_score_4': scores[3] if len(scores) > 3 else None,\n",
    "            'mse_score_5': scores[4] if len(scores) > 4 else None,\n",
    "            'mse_score_6': scores[5] if len(scores) > 5 else None,\n",
    "            'mse_score_7': scores[6] if len(scores) > 6 else None,\n",
    "            'mse_score_8': scores[7] if len(scores) > 7 else None,\n",
    "            'mse_score_9': scores[8] if len(scores) > 8 else None,\n",
    "            'mse_score_10': scores[9] if len(scores) > 9 else None,\n",
    "            'mean_mse_score': mean_score,\n",
    "            'prev_1': prev[0] if len(prev) > 0 else None,\n",
    "            'prev_2': prev[1] if len(prev) > 1 else None,\n",
    "            'prev_3': prev[2] if len(prev) > 2 else None,\n",
    "            'prev_4': prev[3] if len(prev) > 3 else None,\n",
    "            'prev_5': prev[4] if len(prev) > 4 else None,\n",
    "            'prev_6': prev[5] if len(prev) > 5 else None,\n",
    "            'prev_7': prev[6] if len(prev) > 6 else None,\n",
    "            'prev_8': prev[7] if len(prev) > 7 else None,\n",
    "            'prev_9': prev[8] if len(prev) > 8 else None,\n",
    "            'prev_10': prev[9] if len(prev) > 9 else None,\n",
    "            'mean_prev': mean_prev,\n",
    "            'ptev_1': ptev[0] if len(ptev) > 0 else None,\n",
    "            'ptev_2': ptev[1] if len(ptev) > 1 else None,\n",
    "            'ptev_3': ptev[2] if len(ptev) > 2 else None,\n",
    "            'ptev_4': ptev[3] if len(ptev) > 3 else None,\n",
    "            'ptev_5': ptev[4] if len(ptev) > 4 else None,\n",
    "            'ptev_6': ptev[5] if len(ptev) > 5 else None,\n",
    "            'ptev_7': ptev[6] if len(ptev) > 6 else None,\n",
    "            'ptev_8': ptev[7] if len(ptev) > 7 else None,\n",
    "            'ptev_9': ptev[8] if len(ptev) > 8 else None,\n",
    "            'ptev_10': ptev[9] if len(ptev) > 9 else None,\n",
    "            'mean_ptev': mean_ptev,\n",
    "            'oob_1': oob_scores[0] if len(oob_scores) > 0 else None,\n",
    "            'oob_2': oob_scores[1] if len(oob_scores) > 1 else None,\n",
    "            'oob_3': oob_scores[2] if len(oob_scores) > 2 else None,\n",
    "            'oob_4': oob_scores[3] if len(oob_scores) > 3 else None,\n",
    "            'oob_5': oob_scores[4] if len(oob_scores) > 4 else None,\n",
    "            'oob_6': oob_scores[5] if len(oob_scores) > 5 else None,\n",
    "            'oob_7': oob_scores[6] if len(oob_scores) > 6 else None,\n",
    "            'oob_8': oob_scores[7] if len(oob_scores) > 7 else None,\n",
    "            'oob_9': oob_scores[8] if len(oob_scores) > 8 else None,\n",
    "            'oob_10': oob_scores[9] if len(oob_scores) > 9 else None,\n",
    "            'oob_score': mean_oob_score\n",
    "        }\n",
    "        # Append the result dictionary to the results list\n",
    "        results.append(result_dict)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "\n",
    "# Convert the results list to a DataFrame and save it to a CSV file\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/1.clinical\"\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'{df_dir}/dec5_maggie_params_tuning_raw_clinical_oob.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
