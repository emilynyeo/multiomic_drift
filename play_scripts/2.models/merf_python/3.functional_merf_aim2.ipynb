{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from merf import MERF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "sns.set_context(\"poster\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (11,8)\n",
    "from merf.merf import MERF\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from merf.viz import plot_merf_training_stats\n",
    "from em_utils import *\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_plots/3.functional\"\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/3.functional\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- Read data ----------\")\n",
    "func_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/functional/aim2\"\n",
    "test = read_data(func_dir, \"all_clr_testing.csv\")\n",
    "train = read_data(func_dir, \"all_clr_training.csv\")\n",
    "\n",
    "print(\"---------- Read metadata ----------\")\n",
    "m1_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/clinical/transformed/aim2\"\n",
    "met_test = read_data(m1_dir, \"a2_test_samples_standard_clinical.csv\")\n",
    "met_train = read_data(m1_dir, \"a2_train_samples_standard_clinical.csv\")\n",
    "met_full = read_data(m1_dir, \"a2_meta_Transformed_standard_clinical.csv\")\n",
    "met_full_raw = read_data(m1_dir, \"a2_meta_not_Transformed_standard_clinical.csv\")\n",
    "\n",
    "print(\"---------- Columns in meta data ----------\")\n",
    "print(met_test.columns.to_list())\n",
    "print(met_train.columns.to_list())\n",
    "print(\"---------- Columns in meta unnamed ----------\")\n",
    "print(met_test['Unnamed: 0'])\n",
    "print(met_train['Unnamed: 0'])\n",
    "print(\"---------- meta subject_ids ----------\")\n",
    "print(met_test['subject_id'])\n",
    "print(met_train['subject_id'])\n",
    "print(\"---------- Meta record_id ----------\")\n",
    "print(met_test['record_id'])\n",
    "print(met_train['record_id'])\n",
    "\n",
    "print(\"---------- Columns in functional data ----------\")\n",
    "print(test.columns.to_list())\n",
    "print(train.columns.to_list())\n",
    "print(\"---------- Do test and train data have the same columns? ----------\")\n",
    "print(test.columns.to_list() == train.columns.to_list())\n",
    "print(train.columns.to_list() == test.columns.to_list())\n",
    "print(\"---------- WTF is the unnammed ----------\")\n",
    "print(train['Unnamed: 0'])\n",
    "print(test['Unnamed: 0'])\n",
    "print(\"---------- Sample _ IDs ----------\")\n",
    "print(train['SampleID'])\n",
    "print(test['SampleID'])\n",
    "print(\"---------- All Samples ----------\")\n",
    "print(train['all_samples'])\n",
    "print(test['all_samples'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X column into character_id and timepoint\n",
    "print(\"---------- Split SampleID column into character_id and timepoint ---------\")\n",
    "train[['character_id', 'timepoint']] = train['SampleID'].str.split('.', expand=True)\n",
    "test[['character_id', 'timepoint']] = test['SampleID'].str.split('.', expand=True)\n",
    "\n",
    "print(test.columns.to_list())\n",
    "print(train.columns.to_list())\n",
    "\n",
    "# Remove 3 and 18 months \n",
    "train = train[~train['timepoint'].isin(['3', '18'])]\n",
    "test = test[~test['timepoint'].isin(['3', '18'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.columns.to_list())\n",
    "print(train.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit meta data prior to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each meta dataset\n",
    "print(\"---------- Convert metadata to long format ----------\")\n",
    "met_full_long = make_long(met_full_raw)\n",
    "met_full_long['x_t'] = met_full_long['subject_id'].astype(str) + '.' + met_full_long['time'].astype(str)\n",
    "\n",
    "met_train_long = make_long(met_train)\n",
    "met_train_long['x_t'] = met_train_long['subject_id'].astype(str) + '.' + met_train_long['time'].astype(str)\n",
    "\n",
    "met_test_long = make_long(met_test)\n",
    "met_test_long['x_t'] = met_test_long['subject_id'].astype(str) + '.' + met_test_long['time'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print x_t values \n",
    "print(\"taxa character_id\", train['character_id'])\n",
    "print(\"meta x_t\", met_full_long['x_t'])\n",
    "\n",
    "test['time'] = test['timepoint'].replace(['BL', '12m', '6m'], ['0', '12', '6'])\n",
    "test = test[~test['time'].isin(['3m', '18m'])]\n",
    "test['x_t'] = test['character_id'].astype(str) + '.' + test['time'].astype(str)\n",
    "\n",
    "\n",
    "train['time'] = train['timepoint'].replace(['BL', '12m', '6m'], ['0', '12', '6'])\n",
    "train = train[~train['time'].isin(['3m', '18m'])]\n",
    "train['x_t'] = train['character_id'].astype(str) + '.' + train['time'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test x_t\", test['x_t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- Select and prepare metadata for merging ----------\")\n",
    "met_full_long = met_full_long[['x_t', 'outcome_BMI_fnl']]\n",
    "met_test_long = met_test_long[['x_t', 'outcome_BMI_fnl']]\n",
    "met_train_long = met_train_long[['x_t', 'outcome_BMI_fnl']]\n",
    "print(\"full_meta shape = \", met_full_long.shape)\n",
    "\n",
    "print(\"---------- Merge training data ----------\")\n",
    "full_train_tax = train.merge(met_full_long, on='x_t', how='inner')\n",
    "train_tax = train.merge(met_train_long, on='x_t')\n",
    "train_tax = train_tax.drop(['x_t', 'character_id', 'timepoint'], axis=1)\n",
    "print(\"train_tax shape = \", train_tax.shape)\n",
    "print(\"full_train_tax shape = \", full_train_tax.shape)\n",
    "\n",
    "print(\"---------- Merge testing data ----------\")\n",
    "full_test_tax = test.merge(met_full_long, on='x_t', how='inner')\n",
    "test_tax = test.merge(met_test_long, on='x_t')\n",
    "test_tax = test_tax.drop(['x_t', 'character_id', 'timepoint'], axis=1)\n",
    "print(\"test_tax shape = \", test_tax.shape)\n",
    "print(\"full_test_tax shape = \", full_test_tax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tax_no_na = test_tax.dropna()\n",
    "train_tax_no_na = train_tax.dropna()\n",
    "full_train_tax = full_train_tax.dropna()\n",
    "full_test_tax = full_test_tax.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/functional/aim2/\"\n",
    "#test_tax_no_na.to_csv(os.path.join(data_dir, 'test_functional_no_na.csv'), index=False)  # Save test_tax_no_na\n",
    "#train_tax_no_na.to_csv(os.path.join(data_dir, 'train_functional_no_na.csv'), index=False)  # Save train_tax_no_na\n",
    "#full_train_tax.to_csv(os.path.join(data_dir, 'full_train_functional.csv'), index=False)  # Save full_train_tax\n",
    "#full_test_tax.to_csv(os.path.join(data_dir, 'full_test_functional.csv'), index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns full :\", full_train_tax.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- Select predictors for training set ----------\")\n",
    "train_set = full_train_tax\n",
    "X = train_set.drop(['Unnamed: 0', 'SampleID','all_samples', 'character_id', 'timepoint', 'time', 'x_t', 'outcome_BMI_fnl'], axis=1)\n",
    "#X = X.drop(columns=['Unnamed: 0_tax', 'x_t'], errors='ignore')\n",
    "Y = train_set[['outcome_BMI_fnl']]\n",
    "Y = Y['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array\n",
    "clusters_train = train_set['all_samples'].to_numpy() # Get ID variables\n",
    "Z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- Select predictors for test set ----------\")\n",
    "test_set = full_test_tax\n",
    "X_new = test_set.drop(['Unnamed: 0', 'SampleID','all_samples', 'character_id', 'timepoint', 'time', 'x_t', 'outcome_BMI_fnl'], axis=1)\n",
    "X_new = X_new[X.columns]  # Reorder and select columns to match training set\n",
    "X_new = X_new.astype(X.dtypes)  # Ensure data types match\n",
    "X_new = X_new.drop(columns=['Unnamed: 0_tax', 'x_t'], errors='ignore')\n",
    "X_new = X_new.drop(columns=['Unnamed: 0', 'character_id', 'timepoint'], errors='ignore')\n",
    "Y_new = test_set['outcome_BMI_fnl'].to_numpy()  # Convert Y to numeric array\n",
    "clusters_new = pd.Series(test_set['all_samples'])  # Convert to pandas Series\n",
    "# Create random effects matrix with ones\n",
    "Z_new = np.ones((len(X_new), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [0.05, 0.1, 0.15],\n",
    "    'max_iter': [2, 10],\n",
    "    'n_splits': [3, 5, 10]  # Added n_splits for cross-validation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the tuning Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = full_train_tax\n",
    "columns_to_drop = ['Unnamed: 0', 'SampleID','all_samples', 'character_id', 'timepoint', 'time', 'x_t', 'outcome_BMI_fnl']\n",
    "\n",
    "# Create training features\n",
    "X = train_set.drop(columns=columns_to_drop, errors='ignore')\n",
    "y = train_set[['outcome_BMI_fnl']]\n",
    "y = y['outcome_BMI_fnl'].to_numpy() # Convert Y to numeric array\n",
    "clusters = train_set['all_samples'].to_numpy() # Get ID variables\n",
    "z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "# Initialize a list to store the results of each iteration\n",
    "results = []\n",
    "\n",
    "# Loop through all possible combinations of parameters\n",
    "for params in itertools.product(*param_grid.values()):\n",
    "        n_estimators, max_depth, min_samples_split, max_iter, n_splits = params\n",
    "        print(f\"Combination: {params}\\n\")\n",
    "        scores = []\n",
    "        prev = []\n",
    "        ptev = []\n",
    "        oob_scores = []  # Initialize a list to store OOB scores\n",
    "\n",
    "        # K-fold cross-validation with variable n_splits\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # Use .iloc for row selection\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            clusters_train, clusters_test = clusters[train_index], pd.Series(clusters[test_index])\n",
    "            z_train, z_test = z[train_index], z[test_index]\n",
    "            model = MERF(\n",
    "                # Specify the fixed effects model as a Random Forest Regressor\n",
    "                fixed_effects_model=RandomForestRegressor(\n",
    "                    n_estimators=n_estimators,  # Number of trees in the forest\n",
    "                    max_depth=max_depth,  # Maximum depth of each tree\n",
    "                    min_samples_split=min_samples_split,  # Minimum samples required to split an internal node\n",
    "                    n_jobs=1,  # Number of jobs to run in parallel\n",
    "                    oob_score=True  # Whether to use out-of-bag samples to estimate the R^2 on unseen data\n",
    "                ),\n",
    "                # Generalized Linear Model (GLM) early stopping threshold\n",
    "                gll_early_stop_threshold=None,  # No early stopping threshold set\n",
    "                # Maximum number of iterations for the MERF algorithm\n",
    "                max_iterations=max_iter  # Maximum number of iterations to run the MERF algorithm\n",
    "            )\n",
    "            model.fit(X_train.select_dtypes(include=[np.number]), z_train, pd.Series(clusters_train), y_train)\n",
    "            y_pred = model.predict(X_test, z_test, clusters_test)\n",
    "            scores.append(np.mean((y_pred - y_test) ** 2)) # MSE\n",
    "            \n",
    "            # Calculate ptev and prev\n",
    "            total_variance = np.var(y_test) #calculates the total variance of the predicted values\n",
    "            random_effect_variance = np.var(y_test - y_pred)  # Variance of residuals\n",
    "            fixed_effect_variance = total_variance - random_effect_variance\n",
    "\n",
    "            ptev.append(np.mean(fixed_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "            prev.append(np.mean(random_effect_variance / total_variance if total_variance > 0 else 0))\n",
    "\n",
    "            # Calculate OOB score\n",
    "            forest = model.trained_fe_model\n",
    "            oob_score = round(forest.oob_score_*100, 1)  # percent variation\n",
    "            oob_scores.append(oob_score)  # Append OOB score to the list\n",
    "\n",
    "            # Print ptev, prev, and OOB score for the current iteration\n",
    "            print(f\"Combination, ptev: {np.mean(ptev):.4f}, prev: {np.mean(prev):.4f}, OOB Score: {oob_score:.4f}\")\n",
    "\n",
    "        # Calculate the mean of the scores for the current combination of parameters\n",
    "        mean_score = np.mean(scores)\n",
    "        mean_prev = np.mean(prev)\n",
    "        mean_ptev = np.mean(ptev)\n",
    "        mean_oob_score = np.mean(oob_scores)  # Calculate the mean of OOB scores\n",
    "        if mean_score < best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "        # Append the results of the current iteration to the results list\n",
    "        # Create a result dictionary with individual scores and mean scores\n",
    "        result_dict = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'max_iter': max_iter,\n",
    "            'n_splits': n_splits,  # Added n_splits to the result dictionary\n",
    "            'mse_score_1': scores[0] if len(scores) > 0 else None,\n",
    "            'mse_score_2': scores[1] if len(scores) > 1 else None,\n",
    "            'mse_score_3': scores[2] if len(scores) > 2 else None,\n",
    "            'mse_score_4': scores[3] if len(scores) > 3 else None,\n",
    "            'mse_score_5': scores[4] if len(scores) > 4 else None,\n",
    "            'mse_score_6': scores[5] if len(scores) > 5 else None,\n",
    "            'mse_score_7': scores[6] if len(scores) > 6 else None,\n",
    "            'mse_score_8': scores[7] if len(scores) > 7 else None,\n",
    "            'mse_score_9': scores[8] if len(scores) > 8 else None,\n",
    "            'mse_score_10': scores[9] if len(scores) > 9 else None,\n",
    "            'mean_mse_score': mean_score,\n",
    "            'prev_1': prev[0] if len(prev) > 0 else None,\n",
    "            'prev_2': prev[1] if len(prev) > 1 else None,\n",
    "            'prev_3': prev[2] if len(prev) > 2 else None,\n",
    "            'prev_4': prev[3] if len(prev) > 3 else None,\n",
    "            'prev_5': prev[4] if len(prev) > 4 else None,\n",
    "            'prev_6': prev[5] if len(prev) > 5 else None,\n",
    "            'prev_7': prev[6] if len(prev) > 6 else None,\n",
    "            'prev_8': prev[7] if len(prev) > 7 else None,\n",
    "            'prev_9': prev[8] if len(prev) > 8 else None,\n",
    "            'prev_10': prev[9] if len(prev) > 9 else None,\n",
    "            'mean_prev': mean_prev,\n",
    "            'ptev_1': ptev[0] if len(ptev) > 0 else None,\n",
    "            'ptev_2': ptev[1] if len(ptev) > 1 else None,\n",
    "            'ptev_3': ptev[2] if len(ptev) > 2 else None,\n",
    "            'ptev_4': ptev[3] if len(ptev) > 3 else None,\n",
    "            'ptev_5': ptev[4] if len(ptev) > 4 else None,\n",
    "            'ptev_6': ptev[5] if len(ptev) > 5 else None,\n",
    "            'ptev_7': ptev[6] if len(ptev) > 6 else None,\n",
    "            'ptev_8': ptev[7] if len(ptev) > 7 else None,\n",
    "            'ptev_9': ptev[8] if len(ptev) > 8 else None,\n",
    "            'ptev_10': ptev[9] if len(ptev) > 9 else None,\n",
    "            'mean_ptev': mean_ptev,\n",
    "            'oob_1': oob_scores[0] if len(oob_scores) > 0 else None,\n",
    "            'oob_2': oob_scores[1] if len(oob_scores) > 1 else None,\n",
    "            'oob_3': oob_scores[2] if len(oob_scores) > 2 else None,\n",
    "            'oob_4': oob_scores[3] if len(oob_scores) > 3 else None,\n",
    "            'oob_5': oob_scores[4] if len(oob_scores) > 4 else None,\n",
    "            'oob_6': oob_scores[5] if len(oob_scores) > 5 else None,\n",
    "            'oob_7': oob_scores[6] if len(oob_scores) > 6 else None,\n",
    "            'oob_8': oob_scores[7] if len(oob_scores) > 7 else None,\n",
    "            'oob_9': oob_scores[8] if len(oob_scores) > 8 else None,\n",
    "            'oob_10': oob_scores[9] if len(oob_scores) > 9 else None,\n",
    "            'oob_score': mean_oob_score\n",
    "        }\n",
    "        # Append the result dictionary to the results list\n",
    "        results.append(result_dict)\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best score:\", best_score)\n",
    "\n",
    "# Convert the results list to a DataFrame and save it to a CSV file\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/3.functional\"\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(f'{df_dir}/dec6_magggie_params_fine_tuning_results_raw_functional.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
