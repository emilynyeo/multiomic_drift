{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from merf import MERF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools \n",
    "sns.set_context(\"poster\")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (11,8)\n",
    "from merf.merf import MERF\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from merf.viz import plot_merf_training_stats\n",
    "\n",
    "current_dir = os.getcwd() # Get the current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from em_utils import *\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_plots/6.two_timepoints_delta\"\n",
    "df_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/play_scripts/2.models/merf_python/merf_dfs/6.two_timepoints_deltas\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "m1_dir = \"/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/zachs_rerun/drift_fs/csv/all_omic_processed_data/deltas/\"\n",
    "test = read_data(m1_dir, \"feb20_all_delta_test.csv\")\n",
    "train = read_data(m1_dir, \"feb20_all_delta_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train columns :\", train.columns.to_list())\n",
    "print(\"Test columns:\", test.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Weight', 'CRP', 'ghrelin','leptin', 'age.x''peptide_yy', 'tgcyd', 'bmi_prs', 'age.y', \n",
    "                   'outcome_wt_fnl', 'outcome_BMI_fnl', 'Glucose','HOMA_IR', 'Insulin_endo', 'HDL_Total_Direct_lipid', 'LDL_Calculated', 'Triglyceride_lipid']\n",
    "train = train.drop([col for col in columns_to_drop if col in train.columns], axis=1)\n",
    "test = test.drop([col for col in columns_to_drop if col in test.columns], axis=1)\n",
    "\n",
    "print(\"test shape = \", test.shape)\n",
    "print(\"train shape = \", train.shape)\n",
    "\n",
    "test = test.dropna()\n",
    "train = train.dropna()\n",
    "\n",
    "print(\"test shape = \", test.shape)\n",
    "print(\"train shape = \", train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- Select predictors for training set ----------\")\n",
    "train_set = train\n",
    "X = train_set.drop(['BMI', 'subject_id', 'range'], axis=1)\n",
    "Y = train_set[['BMI']]\n",
    "Y = Y['BMI'].to_numpy() # Convert Y to numeric array\n",
    "clusters_train = train_set['subject_id'].to_numpy() # Get ID variables\n",
    "Z = np.ones((train_set.shape[0], 1)) # Create random effects matrix with ones\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"X data: \\n\", X)\n",
    "\n",
    "print(\"---------- Select predictors for test set ----------\")\n",
    "test_set = test\n",
    "X_new = test_set.drop(['BMI', 'subject_id', 'range'], axis=1)\n",
    "X_new = X_new[X.columns]  # Reorder and select columns to match training set\n",
    "X_new = X_new.astype(X.dtypes)  # Ensure data types match\n",
    "\n",
    "Y_new = test_set['BMI'].to_numpy()  # Convert Y to numeric array\n",
    "clusters_new = pd.Series(test_set['subject_id'])  # Convert to pandas Series\n",
    "Z_new = np.ones((len(X_new), 1))\n",
    "\n",
    "print(\"X_new shape: \", X_new.shape)\n",
    "print(\"X_new data: \\n\", X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in fine tuning params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(df_dir, 'feb20_2times_omic_deltas_BMI.csv'))\n",
    "\n",
    "# the line below filters out certain folds\n",
    "# df = df[df['n_splits'] == 5]\n",
    "\n",
    "# Find the row with the lowest mean_mse_score\n",
    "lowest_mse_row = df.loc[df['mean_mse_score'].idxmin()]\n",
    "print(\"First 5 columns for the lowest mean_mse_score:\")\n",
    "print(lowest_mse_row.iloc[:5])\n",
    "\n",
    "# Find the row with the lowest mean_prev_score\n",
    "lowest_prev_row = df.loc[df['mean_prev'].idxmin()]\n",
    "print(\"First 5 columns for the lowest mean_prev_score:\")\n",
    "print(lowest_prev_row.iloc[:5])\n",
    "\n",
    "# Find the row with the lowest mean_prev_score\n",
    "lowest_ptev_row = df.loc[df['mean_ptev'].idxmin()]\n",
    "print(\"First 5 columns for the lowest mean_ptev_score:\")\n",
    "print(lowest_ptev_row.iloc[:5])\n",
    "\n",
    "# Find the row with the highest oob_score\n",
    "highest_oob_row = df.loc[df['oob_score'].idxmax()]\n",
    "print(\"\\nFirst 5 columns for the highest oob_score:\")\n",
    "print(highest_oob_row.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at all the parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the rows for the parameter grids\n",
    "lowest_mse_row = df.loc[df['mean_mse_score'].idxmin()]\n",
    "lowest_prev_row = df.loc[df['mean_prev'].idxmin()]\n",
    "lowest_ptev_row = df.loc[df['mean_ptev'].idxmin()]\n",
    "highest_oob_row = df.loc[df['oob_score'].idxmax()]\n",
    "\n",
    "# Create parameter grids from the extracted rows\n",
    "best_mse_param_grid = {\n",
    "    'n_estimators': [int(lowest_mse_row['n_estimators'])],\n",
    "    #'max_depth': [int(lowest_mse_row['max_depth'])],\n",
    "    'max_depth': [None if pd.isna(lowest_mse_row['max_depth']) else int(lowest_mse_row['max_depth'])],\n",
    "    'min_samples_split': [float(lowest_mse_row['min_samples_split'])],\n",
    "    #'min_samples_split': [float(lowest_mse_row['min_samples_split']) if lowest_mse_row['min_samples_split'] != 0 else 0.0],  # Convert 0 to 0.0\n",
    "    'max_iter': [int(lowest_mse_row['max_iter'])],\n",
    "    'n_splits': [int(lowest_mse_row['n_splits'])]\n",
    "}\n",
    "print(\"Best MSE Parameter Grid:\")\n",
    "print(\"n_estimators:\", best_mse_param_grid['n_estimators'][0])\n",
    "print(\"max_depth:\", best_mse_param_grid['max_depth'][0])\n",
    "print(\"min_samples_split:\", best_mse_param_grid['min_samples_split'][0])\n",
    "print(\"max_iter:\", best_mse_param_grid['max_iter'][0])\n",
    "print(\"n_splits:\", best_mse_param_grid['n_splits'][0])\n",
    "\n",
    "lowest_prev_param_grid = {\n",
    "    'n_estimators': [int(lowest_prev_row['n_estimators'])],\n",
    "    #'max_depth': [int(lowest_prev_row['max_depth'])],\n",
    "    'max_depth': [None if pd.isna(lowest_prev_row['max_depth']) else int(lowest_prev_row['max_depth'])],\n",
    "    'min_samples_split': [float(lowest_prev_row['min_samples_split'])],\n",
    "    'max_iter': [int(lowest_prev_row['max_iter'])],\n",
    "    'n_splits': [int(lowest_prev_row['n_splits'])]\n",
    "}\n",
    "print(\"\\nLowest Prev Parameter Grid:\")\n",
    "print(\"n_estimators:\", lowest_prev_param_grid['n_estimators'][0])\n",
    "print(\"max_depth:\", lowest_prev_param_grid['max_depth'][0])\n",
    "print(\"min_samples_split:\", lowest_prev_param_grid['min_samples_split'][0])\n",
    "print(\"max_iter:\", lowest_prev_param_grid['max_iter'][0])\n",
    "print(\"n_splits:\", lowest_prev_param_grid['n_splits'][0])\n",
    "\n",
    "lowest_ptev_param_grid = {\n",
    "    'n_estimators': [int(lowest_ptev_row['n_estimators'])],\n",
    "    #'max_depth': [int(lowest_ptev_row['max_depth'])],\n",
    "    'max_depth': [None if pd.isna(lowest_ptev_row['max_depth']) else int(lowest_ptev_row['max_depth'])],\n",
    "    'min_samples_split': [float(lowest_ptev_row['min_samples_split'])],\n",
    "    'max_iter': [int(lowest_ptev_row['max_iter'])],\n",
    "    'n_splits': [int(lowest_ptev_row['n_splits'])]\n",
    "}\n",
    "print(\"\\nLowest PTEV Parameter Grid:\")\n",
    "print(\"n_estimators:\", lowest_ptev_param_grid['n_estimators'][0])\n",
    "print(\"max_depth:\", lowest_ptev_param_grid['max_depth'][0])\n",
    "print(\"min_samples_split:\", lowest_ptev_param_grid['min_samples_split'][0])\n",
    "print(\"max_iter:\", lowest_ptev_param_grid['max_iter'][0])\n",
    "print(\"n_splits:\", lowest_ptev_param_grid['n_splits'][0])\n",
    "\n",
    "highest_oob_param_grid = {\n",
    "    'n_estimators': [int(highest_oob_row['n_estimators'])],\n",
    "    'max_depth': [None if pd.isna(highest_oob_row['max_depth']) else int(highest_oob_row['max_depth'])],\n",
    "    'min_samples_split': [float(highest_oob_row['min_samples_split'])],\n",
    "    'max_iter': [int(highest_oob_row['max_iter'])],\n",
    "    'n_splits': [int(highest_oob_row['n_splits'])]\n",
    "}\n",
    "print(\"\\Highest OOB Parameter Grid:\")\n",
    "print(\"n_estimators:\", highest_oob_row['n_estimators'])\n",
    "print(\"max_depth:\", highest_oob_row['max_depth'])\n",
    "print(\"min_samples_split:\", highest_oob_row['min_samples_split'])\n",
    "print(\"max_iter:\", highest_oob_row['max_iter'])\n",
    "print(\"n_splits:\", highest_oob_row['n_splits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up different merfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MERF models for each parameter grid\n",
    "mse_merf = MERF(fixed_effects_model =\n",
    "           RandomForestRegressor(n_estimators = best_mse_param_grid['n_estimators'][0], \n",
    "                                max_depth = best_mse_param_grid['max_depth'][0], \n",
    "                                min_samples_split = best_mse_param_grid['min_samples_split'][0], \n",
    "                                n_jobs = 1, \n",
    "                                oob_score= True),\n",
    "            gll_early_stop_threshold=None,\n",
    "            max_iterations = best_mse_param_grid['max_iter'][0])\n",
    "\n",
    "prev_merf = MERF(fixed_effects_model =\n",
    "           RandomForestRegressor(n_estimators = lowest_prev_param_grid['n_estimators'][0], \n",
    "                                max_depth = lowest_prev_param_grid['max_depth'][0], \n",
    "                                min_samples_split = lowest_prev_param_grid['min_samples_split'][0], \n",
    "                                n_jobs = 1, \n",
    "                                oob_score= True),\n",
    "            gll_early_stop_threshold=None,\n",
    "            max_iterations = lowest_prev_param_grid['max_iter'][0])\n",
    "\n",
    "ptev_merf = MERF(fixed_effects_model =\n",
    "           RandomForestRegressor(n_estimators = lowest_ptev_param_grid['n_estimators'][0], \n",
    "                                max_depth = lowest_ptev_param_grid['max_depth'][0], \n",
    "                                min_samples_split = lowest_ptev_param_grid['min_samples_split'][0], \n",
    "                                n_jobs = 1, \n",
    "                                oob_score= True),\n",
    "            gll_early_stop_threshold=None,\n",
    "            max_iterations = lowest_ptev_param_grid['max_iter'][0])\n",
    "\n",
    "oob_merf = MERF(fixed_effects_model =\n",
    "           RandomForestRegressor(n_estimators = highest_oob_param_grid['n_estimators'][0], \n",
    "                                max_depth = highest_oob_param_grid['max_depth'][0], \n",
    "                                min_samples_split = highest_oob_param_grid['min_samples_split'][0], \n",
    "                                n_jobs = 1, \n",
    "                                oob_score= True),\n",
    "            gll_early_stop_threshold=None,\n",
    "            max_iterations = highest_oob_param_grid['max_iter'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run Merfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- RUN MERF RAW WITH TUNING PARAMETERS ðŸŒ± ----------\")\n",
    "mrf_mse = mse_merf.fit(X.select_dtypes(include=[np.number]), \n",
    "        Z, \n",
    "        pd.Series(clusters_train), \n",
    "        Y)\n",
    "\n",
    "mrf_prev = prev_merf.fit(X.select_dtypes(include=[np.number]), \n",
    "        Z, \n",
    "        pd.Series(clusters_train), \n",
    "        Y)\n",
    "\n",
    "mrf_ptev = ptev_merf.fit(X.select_dtypes(include=[np.number]), \n",
    "        Z, \n",
    "        pd.Series(clusters_train), \n",
    "        Y)\n",
    "\n",
    "mrf_oob = oob_merf.fit(X.select_dtypes(include=[np.number]), \n",
    "                Z, \n",
    "                pd.Series(clusters_train), \n",
    "                Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions using fitted models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the fitted model\n",
    "clusters_new = pd.Series(test_set['subject_id'])\n",
    "X_new = test_set.drop(['BMI', 'subject_id', 'range'], axis=1)\n",
    "y_hat_new_mse = mrf_mse.predict(X_new, Z_new, clusters_new)\n",
    "forest_mse = mrf_mse.trained_fe_model\n",
    "oob_score_mse = round(forest_mse.oob_score_*100, 1)\n",
    "\n",
    "y_hat_new_prev = mrf_prev.predict(X_new, Z_new, clusters_new)\n",
    "forest_prev = mrf_prev.trained_fe_model\n",
    "oob_score_prev = round(forest_prev.oob_score_*100, 1)\n",
    "\n",
    "y_hat_new_ptev = mrf_ptev.predict(X_new, Z_new, clusters_new)\n",
    "forest_ptev = mrf_ptev.trained_fe_model\n",
    "oob_score_ptev = round(forest_ptev.oob_score_*100, 1)\n",
    "\n",
    "y_hat_new_oob = mrf_oob.predict(X_new, Z_new, clusters_new)\n",
    "forest_oob = mrf_oob.trained_fe_model\n",
    "oob_score_tuned = round(forest_oob.oob_score_*100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predicted vs actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predicted_vs_actual(y_hat_new_mse, Y_new,\n",
    "                         output_dir, 'feb20_all_predicted_vs_actual_mse_tuned.png', \n",
    "                         best_mse_param_grid, oob_score_mse, '#F88F79', '2 time deltas mse predicted vs actual BMI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predicted_vs_actual(y_hat_new_prev, Y_new,\n",
    "                         output_dir, 'Feb20_all_predicted_vs_actual_prev_tuned.png', \n",
    "                         lowest_prev_param_grid, oob_score_prev, '#F0F879', '2 time deltas predicted vs actual BMI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(Y_new, y_hat_new_ptev) #(Y_true, Y_pred)\n",
    "plot_predicted_vs_actual(y_hat_new_ptev, Y_new,\n",
    "                         output_dir, 'feb20_all_predicted_vs_actual_ptev_tuned.png', \n",
    "                         lowest_ptev_param_grid, oob_score_ptev, '#ACF0F8', '2 time deltas predicted vs actual BMI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(Y_new, y_hat_new_oob) #(Y_true, Y_pred)\n",
    "plot_predicted_vs_actual(y_hat_new_oob, Y_new,\n",
    "                         output_dir, 'feb20_all_predicted_vs_actual_oob_tuned.png', \n",
    "                         highest_oob_param_grid, oob_score_tuned, '#86B874', '2 time deltas predicted vs actual BMI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at feature importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse\n",
    "mse_forest = mrf_mse.trained_fe_model\n",
    "mse_feature_names = mse_forest.feature_names_in_\n",
    "mse_feature_importances = mse_forest.feature_importances_\n",
    "\n",
    "print(\"Top 20 ft imp for best MSE parameters:\")\n",
    "for name, importance in zip(mse_feature_names[:20], \n",
    "                            mse_feature_importances[:20]):\n",
    "    print(f\"{name}: {importance}\")\n",
    "\n",
    "plot_top_20_feature_importances(mse_feature_names, mse_feature_importances, \n",
    "                         output_dir, 'feb20_deltas_2time_mse_feature_importances_all', '#F88F79')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_forest = prev_merf.trained_fe_model\n",
    "prev_feature_names = prev_forest.feature_names_in_\n",
    "prev_feature_importances = prev_forest.feature_importances_\n",
    "\n",
    "print(\"Top 20 ft imp for best PREV parameters:\")\n",
    "for name, importance in zip(prev_feature_names[:20], \n",
    "                            prev_feature_importances[:20]):\n",
    "    print(f\"{name}: {importance}\")\n",
    "\n",
    "plot_top_20_feature_importances(prev_feature_names, prev_feature_importances, \n",
    "                         output_dir, 'feb20_deltas_2time_prev_feature_importances_all', '#F0F879')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptev_forest = ptev_merf.trained_fe_model\n",
    "ptev_feature_names = ptev_forest.feature_names_in_\n",
    "ptev_feature_importances = ptev_forest.feature_importances_\n",
    "\n",
    "print(\"Top 20 feature importances for best PTEV parameters:\")\n",
    "for name, importance in zip(ptev_feature_names[:20], \n",
    "                            ptev_feature_importances[:20]):\n",
    "    print(f\"{name}: {importance}\")\n",
    "\n",
    "plot_top_20_feature_importances(ptev_feature_names, ptev_feature_importances, \n",
    "                         output_dir, 'feb20__ptev_feature_importances_all', '#ACF0F8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oob_forest = oob_merf.trained_fe_model\n",
    "oob_feature_names = oob_forest.feature_names_in_\n",
    "oob_feature_importances = oob_forest.feature_importances_\n",
    "\n",
    "print(\"Top 20 ft imp for best OOB parameters:\")\n",
    "for name, importance in zip(oob_feature_names[:20], \n",
    "                            oob_feature_importances[:20]):\n",
    "    print(f\"{name}: {importance}\")\n",
    "\n",
    "plot_top_20_feature_importances(oob_feature_names, oob_feature_importances, \n",
    "                         output_dir, 'feb20_deltas_2time_oob_feature_importances_all', '#86B874')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make merged plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate R-squared values for each model\n",
    "from sklearn.metrics import r2_score\n",
    "color_palette = ['#F88F79', '#F0F879', '#ACF0F8', '#86B874']\n",
    "# Calculate R-squared for each model\n",
    "r2_mse = r2_score(Y_new, y_hat_new_mse)\n",
    "r2_prev = r2_score(Y_new, y_hat_new_prev)\n",
    "r2_ptev = r2_score(Y_new, y_hat_new_ptev)\n",
    "r2_oob = r2_score(Y_new, y_hat_new_oob)\n",
    "\n",
    "# Store R-squared values in a dictionary for plotting\n",
    "r2_values = {\n",
    "    'MSE Model': r2_mse,\n",
    "    'Prev Model': r2_prev,\n",
    "    'PTEV Model': r2_ptev,\n",
    "    'OOB Model': r2_oob\n",
    "}\n",
    "\n",
    "# Plot R-squared values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(r2_values.keys(), \n",
    "        r2_values.values(), \n",
    "        color=color_palette)\n",
    "plt.ylabel('R-squared Value')\n",
    "plt.title('R-squared Comparison of MERF Models')\n",
    "plt.ylim(0, 0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(os.path.join(output_dir, \n",
    "                         'feb20_r_squared_comparison_merf_models.png'), \n",
    "                         dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare top feature importances between models\n",
    "color_palette = ['#F88F79', '#F0F879', '#ACF0F8', '#86B874']\n",
    "importances = {\n",
    "    'MSE': mse_feature_importances,\n",
    "    'Prev': prev_feature_importances,\n",
    "    'PTEV': ptev_feature_importances,\n",
    "    'OOB': oob_feature_importances\n",
    "}\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "importances_df = pd.DataFrame(importances, index=mse_feature_names)\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "importances_df = importances_df.sort_values(by=importances_df.columns.tolist(), ascending=False)\n",
    "\n",
    "# Plot top feature importances\n",
    "top_n = 20  # Number of top features to display\n",
    "importances_df.head(top_n).plot(kind='bar', \n",
    "                                figsize=(12, 8),\n",
    "                                color=color_palette)\n",
    "plt.title('Top Feature Importances Comparison')\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.xticks(rotation=45, ha='right')  # Adjusted rotation and horizontal alignment to reduce overlap\n",
    "plt.legend(title='Models', \n",
    "           loc='upper right', \n",
    "           fontsize='small')  # Adjusted legend position to top right corner and made it smaller\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \n",
    "                         'feb20_top_feature_importances_comparison.png'), \n",
    "                         dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
