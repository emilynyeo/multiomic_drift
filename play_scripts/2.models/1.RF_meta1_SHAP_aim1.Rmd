---
title: "Meta data random forest and shap"
author: "Emily Yeo"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
pacman::p_load(knitr, data.table, dplyr, tidyr, tableone, kableExtra, readxl,
               readr, car, RColorBrewer, gridExtra, mlbench, earth, ggplot2, 
               AppliedPredictiveModeling, caret, reshape2, corrplot, stringr,
               summarytools, grid, mice, plyr, mlmRev, cowplot, compositions,
               jtools, broom, patchwork, phyloseq, microbiome, glmnet, ISLR,
               MicrobiomeStat, ANCOMBC, ape, vegan, zCompositions, janitor,
               treeshap, Metrics, randomForest, xgboost, Ckmeans.1d.dp, caret,
               SHAPforxgboost, doParallel, parallel)
```

# Read in Aim 1 meta

```{r}
m1_dir <- "/Users/emily/projects/research/Stanislawski/comps/mutli-omic-predictions/data/clinical/transformed/aim1"

# Create the full file path using paste
test <- read.csv(paste(m1_dir, "test_samples_standard_clinical.csv", sep = "/"))
train <- read.csv(paste(m1_dir, "train_samples_standard_clinical.csv", sep = "/"))
all_m <- read.csv(paste(m1_dir, "a1_meta_Transformed_standard_clinical.csv", sep = "/"))
```

# Working on models {.tabset}

There seem to be a million ways to do this. So here I am exploring it

## randomForest package

<https://modeloriented.github.io/treeshap/>

```{r}
rf <-randomForest(outcome_BMI_fnl_BL~.,data=train, ntree=500) 
print(rf)
```

In this case, the number of variables tried at each split is based on
the following formula: -1 is used as dataset contains dependent variable
as well.

```{r}
floor(sqrt(ncol(train) - 1))
```

The number of variables selected at each split is denoted by mtry in
randomforest function. Step III : Find the optimal mtry value Select
mtry value with minimum out of bag(OOB) error.

```{r}
mtry <- tuneRF(train[4:12],train$outcome_BMI_fnl_BL, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
```

In this case, mtry = 2 is the best mtry as it has least OOB error. mtry
= 2 was also used as default mtry.

Parameters in **tuneRF** function above

-   The stepFactor specifies at each iteration, mtry is inflated (or
    deflated) by this value

-   The improve specifies the (relative) improvement in OOB error must
    be by this much for the search to continue

-   The trace specifies whether to print the progress of the search

-   The plot specifies whether to plot the OOB error as function of mtry

```{r}
set.seed(71)
rf <-randomForest(outcome_BMI_fnl_BL~.,data=train[4:13], 
                  mtry=best.m, 
                  importance=TRUE,
                  ntree=500)
print(rf)
#Evaluate variable importance
importance(rf)
varImpPlot(rf)
```

For each tree, the prediction accu- racy on the out-of-bag portion of
the data is recorded. Then the same is done after permuting each
predictor variable. The difference between the two accuracies are then
averaged over all trees, and normalized by the standard error. For
regression, the MSE is computed on the out-of-bag data for each tree,
and then the same computed after permuting a variable. The differences
are averaged and normalized by the standard error. If the standard error
is equal to 0 for a variable, the division is not done.â€?
varImp.randomForest is a simple wrapper around the importance function
from that package. Similarly, for RandomForest objects, varImp is a
wrapper around varimp in the party package.

### Prediction and Calculate Performance Metrics

```{r}
# Get predictions
pred1 <- predict(rf)

# Evaluate regression performance
rmse_value <- rmse(train$outcome_BMI_fnl_BL, pred1)
print(paste("RMSE:", rmse_value))

# Fit a linear model between actual and predicted 
model <- lm(pred1 ~ train$outcome_BMI_fnl_BL)
plot(train$outcome_BMI_fnl_BL, pred1, 
     xlab = "Outcome BMI", 
     ylab = "Predicted Values", 
     main = "Scatter Plot with Linear Trend Line") +
  abline(model, col = "blue", lwd = 2)
```

## RF with Caret

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           repeats = 10, ## k-fold repeated ten times
                           p = 0.80,
                           verboseIter = TRUE,
                           summaryFunction = defaultSummary, # report performance 
                           selectionFunction = "best", # select optimal tuning
                           savePredictions = "final") # "final" saves the predictions for the optimal tuning parameters.

gbmFit1 <- train(outcome_BMI_fnl_BL ~ ., data = train[4:13],
                 method = "rf",
                 trControl = fitControl,
                 verbose = TRUE)

gbmFit1
```

```{r}
plot(gbmFit1)
```

Two predictors is definitely best. This matches Boruta SHAPs suggestion at the end

### What is the best mtry parameter

```{r}
gbmFit1$bestTune
head(gbmFit1$results)
```

mtry of 2 seems to be best. 

### Variable Importance

```{r}
gbmImp <- varImp(gbmFit1, scale = TRUE)
plot(gbmImp)
```

HOMA_IR and Insulin seem to be teh best variables. This is different from BORUTA SHAP at the end of this script, which suggests Insulin and Triglycerides. 

### Feature selections

Using simple backwards selection, a.k.a. recursive feature elimination
(RFE), algorithm

```{r}
# define the control using a random forest selection function
# library(doParallel)
# # Set up a parallel backend
# cl <- makeCluster(detectCores() - 1)  # Leave one core free
# registerDoParallel(cl)

control <- rfeControl(functions=rfFuncs, 
                      method="cv", 
                      number=10,
                      repeats = 10,
                      p = 0.80,
                      returnResamp = "final",
                      saveDetails = TRUE)

# run the RFE algorithm
results <- rfe(train[,4:12],
               train[,13],
               sizes=c(4:13),
               rfeControl=control)

print(results)
```

The plot may highlight the optimal number of features that gives the
best model performance. Here it seems 4 variables would be best. This is different from 2. 

```{r}
plot(results, type=c("g", "o")) # plot the results
```

### Changing tolerance

Tolerance selects the least complex model within some % tolerance of the
best value. Less complex model based on (x-xbest)/xbestx 100, which is
the % difference. For example, to select parameter values based on a 12%
loss of performance:

## Using mtry grid search algorithms

From this tutorial:
<https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/>

```{r}
# Don't know why but apparently I need cores
# stopCluster(cl)
detectCores(all.tests = FALSE, logical = TRUE)
cl <- makeCluster(2)
cl
registerDoParallel(cl)

dataset <- train
x <- dataset[,4:12]
y <- dataset[,13]

# Create model with default paramters
control <- trainControl(method="repeatedcv", number=10, repeats=10)
seed <- 7
metric <- "RMSE"
set.seed(seed)
mtry <- sqrt(ncol(x))
tunegrid <- expand.grid(.mtry=mtry)

rf_default <- train(outcome_BMI_fnl_BL ~., data=dataset[4:13], 
                    method="rf", 
                    metric=metric, 
                    tuneGrid=tunegrid, 
                    trControl=control)

print(rf_default)
```

With grid search algorithm, the number of variables at each split seems to be 3. 

### Rtrying wuth random search mtry

Now using random search instead of grid search it seems the best mtry value is 2, so not 3. 

```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(seed)
mtry <- sqrt(ncol(x))
rf_random <- train(outcome_BMI_fnl_BL~., data=dataset[4:13], 
                   method="rf", 
                   metric=metric, 
                   tuneLength=15, 
                   trControl=control)
print(rf_random)
plot(rf_random)
```

### tuning using grid search

Using this method. It appears 2 is the best mtry parameter

```{r message=FALSE, warning=FALSE}
control <- trainControl(method="repeatedcv", 
                        number=10, 
                        repeats=10, search="grid")
set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:15))
rf_gridsearch <- train(outcome_BMI_fnl_BL~., data=dataset[4:13], 
                       method="rf", metric=metric, 
                       tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
```

## Compare
Compare the 3 methofs for finding parameters above. 
1. grid search algorithm
2. RF default model
3. RF random search

```{r}
rf_gridsearch[["finalModel"]]
rf_default[["finalModel"]]
rf_random[["finalModel"]]
```

## Tune Manually

I want to keep using caret because it provides a direct point of
comparison to our previous models (apples to apples, even the same data
splits) and because of the repeated cross validation test reduces the severity of overfitting.

One approach is to create many caret models for our algorithm and pass
in a different parameters directly to the algorithm manually. Here I am
looking at different ntree values while keeping mtry constant.

```{r}
# Manual Search
control <- trainControl(method="repeatedcv", 
                        number=10, 
                        repeats=10, 
                        search="grid")

tunegrid <- expand.grid(.mtry=c(sqrt(ncol(x))))
modellist <- list()

# What is the best ntree value 
for (ntree in c(500, 1000, 1500, 2000, 2500)) {
	set.seed(seed)
	fit <- train(outcome_BMI_fnl_BL~., data=dataset[4:13], 
	             method="rf", 
	             metric=metric, 
	             tuneGrid=tunegrid, 
	             trControl=control, 
	             ntree=ntree)
	key <- toString(ntree)
	modellist[[key]] <- fit
}
# compare results
results <- resamples(modellist)
```

##### Compare the models

```{r}
summary(results)
```

-   Model 1000 has the best MAE, indicating it had the smallest average
    error in predictions.

-   Model 2500 has the best RMSE, suggesting it performs well in terms
    of variance and larger errors.

-   Model 2000 has the highest R-squared, which shows it explains more
    variability in the data compared to the others. Conclusion

No single model is the best across all metrics. However, if you
prioritize minimizing average errors, Model 1000 is the best choice. If
you're more concerned with overall predictive performance and capturing
variability, Model 2000 might be preferable. If the goal is to minimize
the impact of larger errors, then Model 2500 would be the best option. They are all very similar though. So for computation, sticking with 500 seems good. 

```{r}
dotplot(results)
```

## Extend Caret Algorithms

Not feeling like this is something to dabble with right

#  {.unnumbered}

# Baruta and SHAP {.tabset}

This is still something I'm figuring out. But doesn't look too bad.

## Boruta

You could consider increasing the maxRuns parameter if tentative
features are left. However, note that you can also provide values of
mtry and ntree parameters, which will be passed to randomForest()
function.

```{r}
library(Boruta)
set.seed(111)
boruta.bank_train <- Boruta(outcome_BMI_fnl_BL~., 
                            data = train[4:13], 
                            doTrace = 2)
print(boruta.bank_train)
```

Double check any values that were left in tentative selection categories:

The boruta package also contains a TentativeRoughFix() function, which
can be used to fill missing decisions by simple comparison of the median
feature Z-score with the median Z-score of the most important shadow
feature:

```{r}
#take a call on tentative features
boruta.bank <- TentativeRoughFix(boruta.bank_train)
print(boruta.bank)
```

Plot features selected vs. not-selected features:

```{r}
plot(boruta.bank, xlab = "", xaxt = "n")

lz <- lapply(1:ncol(boruta.bank$ImpHistory), 
             function(i) {
  boruta.bank$ImpHistory[is.finite(boruta.bank$ImpHistory[, i]), i]
})

names(lz) <- colnames(boruta.bank$ImpHistory)
Labels <- sort(sapply(lz, median))

# Create the axis without labels
axis(side = 1, at = 1:ncol(boruta.bank$ImpHistory), labels = FALSE)

# Add labels at a 45-degree angle
text(x = 1:ncol(boruta.bank$ImpHistory), 
     y = par("usr")[3] - 0.5,  # Adjust the y position as needed
     labels = names(Labels), 
     srt = 45, 
     adj = 1, 
     xpd = TRUE, 
     cex = 0.7)
```

The y axis label Importance represents the Z score of every feature in
the shuffled dataset.

The blue boxplots correspond to minimal, average and maximum Z score of
a shadow feature, while the red and green boxplots represent Z scores of
rejected and confirmed features, respectively. As you can see the red
boxplots have lower Z score than that of maximum Z score of shadow
feature which is precisely the reason they were put in unimportant
category.

Summary of confirmed features:

```{r}
bank_df <- attStats(boruta.bank)
print(bank_df)
```

## BORUTA and SHAP

Based on this tutorial:
<https://amirali-n.github.io/BorutaFeatureSelectionWithShapAnalysis/> ,
which in turn relies on this :
<https://amirali-n.github.io/ExtremeGradientBoosting/>

Assuming a tunned xgBoost algorithm is already fitted to a training data
set, (e.g., look at my own implementation) the next step is to identify
feature importances. Although, feature importances can be evalutated
directly from the boosted trees, these importances have been shown to be
local and inconsistent; see Scott Lundberg et. al. (2019).

Scott Lundbergâ€™s paper proposed a new method to interpret the result of
machine learning algorithms, particularyly boosting methods, that
produce feature importance scores. They show that feature importance,
although averaged over boosted trees, may be inconsistent for different
observations. To overcome this inconsistency, they propose a SHAP score
inspired by Shapley values which combines different explanation models,
e.g., LIME, with Shapley values. The result is a global feature
importance score that is consistent across different test sets.

However, other than to arbitrarily select an importance threshold beyond
which features are considered unimportant, SHAP analysis does not offer
an algorithmic way to filter a large feature set to a limited set of
important features.

To that end, a selection wrapper algorithm known as Boruta is proposed
that iterates over an extended set of features and judges their
importance. In each iteration of the algorithm, a number of features are
copied by shuffling their values. This is used to fit another learner
and to re-evaluate feature importances. If importance of an original
feature is significantly greater than its shuffled copy, that features
is deemed important.

## First run the X boost Model

Before "SHAP'ing"
For tunning the xgboost hyperparameters, the caret grid search method is
used.

Learning rate or eta, maximum depth of trees, and minimum child weight
which measures the number of instances in a node before the algorithm
decides to partition before. gamma denotes the minimum loss reduction
required to make a further partition. subsample and colsample_bytree
indicate the proportion of the training data set and features,
respectively, used in training the algorithm. Before training the
xgboost model, a parallel backend must be registered.

```{r}
xgb.grid=expand.grid(nrounds=100, 
                     eta=seq(0.1, 1, 0.2),
                     max_depth=c(3, 5, 10),
                     gamma = 0, 
                     subsample = 0.7,
                     min_child_weight = c(1, 3, 5), 
                     colsample_bytree = 1)
library(parallel)
library(doParallel)
myCl=makeCluster(detectCores()-4)
registerDoParallel(myCl)

xgb.control=trainControl(method = "cv",
                         number = 10,
                         repeats = 10,
                         verboseIter = TRUE,
                         returnData = FALSE,
                         returnResamp = "none",
                         classProbs = FALSE,
                         allowParallel = TRUE)

xgb.train = train(x = train[4:13],
                  y = train$outcome_BMI_fnl_BL,
                  trControl = xgb.control,
                  tuneGrid = xgb.grid,
                  method = "xgbTree")
                  #scale_pos_weight=sumwneg/sumwpos)

stopCluster(myCl)
```

The result of the grid search for tunning the xgBoost hyperparamaters
are given below. This is the best configuration of The best
configuration of eta, max_depth, and min_child_weight is given by:

```{r}
xgb.train$bestTune
```

Using these parameters, a cross-validated training is done with
nrounds=500 to identify the best iteration, i.e., nrounds. The algorithm
can stop early if the test accuracy is not improved for
early_stopping_rounds number of iterations

```{r}
label <- train$outcome_BMI_fnl_BL  # Extract the target variable
# Convert the feature data frame to a matrix
train_matrix <- as.matrix(train[4:12])

# Set up parameters
params <- list(
  "eta" = xgb.train$bestTune$eta,
  "max_depth" = xgb.train$bestTune$max_depth,
  "gamma" = xgb.train$bestTune$gamma,
  "min_child_weight" = xgb.train$bestTune$min_child_weight,
  "nthread" = 4,
  "objective" = "reg:squarederror")
```

**eta:** The learning rate, which controls how much to change the model
in response to the estimated error each time the model weights are
updated.

**max_depth:** The maximum depth of a tree, which can prevent
overfitting by limiting the model complexity.

**gamma:** A regularization parameter that specifies the minimum loss
reduction required to make a further partition on a leaf node.

**min_child_weight:** Minimum sum of instance weight (hessian) needed in
a child. This parameter controls overfitting.

**nthread:** Number of threads to use for training, which can speed up
the training process. objective: Specifies the learning task and
corresponding objective function; here, it's set for regression tasks.

```{r}
# Perform cross-validation
xgb.crv <- xgb.cv(
  params = params,
  data = train_matrix,
  label = label,
  nrounds = 500, # The maximum number of boosting rounds to be run.
  nfold = 10, # folds for cross-validation
  showsd = TRUE, # Shows standard deviation of evaluation metric across folds
  metrics = "rmse",
  stratified = FALSE, # generally used for classification
  verbose = TRUE,
  print_every_n = 1L,
  early_stopping_rounds = 50)
```

Highest accuracy on the 12 th iteration: 
```{r}
xgb.crv$best_iteration
```

#### Feature Importance with x boost

To produce feature importances, an instance of the xgBoost learner is run with the optimized parameters of the grid search xgb.train and the cross-validated training xgb.crv.

```{r}
xgb.mod=xgboost(data = train_matrix, 
                label = label, 
                max.depth=xgb.train$bestTune$max_depth, 
                eta=xgb.train$bestTune$eta, 
                nthread=4, 
                min_child_weight=xgb.train$bestTune$min_child_weight,
                #scale_pos_weight=sumwneg/sumwpos, 
                eval_metric="rmse", 
                nrounds=xgb.crv$best_iteration, 
                objective="reg:squarederror")
```

The top 5 features with the highest importance are selected here:

```{r}
importance=xgb.importance(feature_names = colnames(train[4:12]), 
                          model = xgb.mod)
importance$Feature[1:5]
```

#### Plot top 5 xboost features 

The top 20 feautre importances are plotted here. xgBoost records three measures of importance for trees;  

- Gain  which measure the contribution of each feature to optimization of the objective function, 

- Cover  counts the number of observations assigned to the feature, and  

- Weight  which denotes the number of times the feature was selected for a tree. 

The following plot shows the importances with respect to Gain and has undergon a little ggplot treatment.The clustering shows the variables of similar importance. 

```{r}
# relabel features for the plot 
feature.label=importance$Feature[1:5]
feature.label=c("BL Insulin", "BL TGs Lipid", "BL Glucose", "Age", "BL HOMA IR")

(gg=xgb.ggplot.importance(importance_matrix = importance[1:5,],
                          rel_to_first = FALSE,
                          n_clusters = c(1:3)))
```

## BORUTA on x boost RF model

Now we can move on to Boruta, feature selection and SHAP. 
https://amirali-n.github.io/BorutaFeatureSelectionWithShapAnalysis/

SHAP analysis does not offer an algorithmic way to filter a large feature set to a limited set of important features.

Boruta iterates over an extended set of features and judges their importance. In each iteration of the algorithm, a number of features are copied by shuffling their values. This is used to fit another learner and to re-evaluate feature importances. If importance of an original feature is significantly greater than its shuffled copy, that features is deemed important.

```{r}
xgb.boruta=Boruta(train[4:12],
                  y=train$outcome_BMI_fnl_BL,
                  maxRuns=100, 
                  doTrace=2,
                  holdHistory=TRUE,
                  getImp=getImpXgboost,
                  max.depth=xgb.train$bestTune$max_depth, 
                  eta=xgb.train$bestTune$eta, 
                  nthread=4, 
                  min_child_weight=xgb.train$bestTune$min_child_weight,
                  #scale_pos_weight=sumwneg/sumwpos, 
                  #eval_metric="auc", 
                  eval_metric="rmse", 
                  #eval_metric="logloss",
                  gamma=xgb.train$bestTune$gamma,
                  nrounds=xgb.crv$best_iteration, 
                  objective="reg:squarederror",
                  tree_method="hist",
                  lambda=0,
                  alpha=0)
```
Borutas selected variables: 

```{r}
#extract Boruta's decision
boruta_dec=attStats(xgb.boruta)
boruta_dec[boruta_dec$decision!="Rejected",]
```

SHAP score inspired by Shapley values which combines different explanation models, e.g., LIME, with Shapley values. The result is a global feature importance score that is consistent across different test sets

```{r}
#get the names of each feature
#imp_features=row.names(boruta_dec)[which(boruta_dec$decision!="Rejected")]
#get feature importance history
boruta.imp.df=as.data.frame(xgb.boruta$ImpHistory)
#keep only confirmed and tentative features
#boruta.imp.df=boruta.imp.df[,names(boruta.imp.df)%in%imp_features]
#transform the data to a data frame with two columns: feature and importance value
boruta.imp.df=melt(boruta.imp.df)
#create a data frame by adding the decision for each feature as well
boruta.imp.df=cbind.data.frame(boruta.imp.df, 
                               decision=boruta_dec$decision[match(boruta.imp.df$variable, 
                                                                 row.names(boruta_dec))])
#reorder features data frame by the importance median value
feature_order=with(boruta.imp.df, 
                   reorder(variable, 
                           value, 
                           median, 
                           order = TRUE))
boruta.imp.df$variable=factor(boruta.imp.df$variable, 
                              levels = levels(feature_order))

# Create the box plot
ggplot(boruta.imp.df, aes(x = variable, y = value, fill = decision)) +
  geom_boxplot(outlier.shape = NA) +  # Avoid plotting outliers
  coord_flip() +  # Flip coordinates for better readability
  labs(title = "Boruta Importance Evaluation",
       x = "Features",
       y = "Importance Value") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10),  # Adjust text size
        legend.title = element_blank())  # Remove legend title
```

## SHAP time

Boruta is very effective in reducing the number of features from more than 700 to just 10. However, these importances may not be consistent with respect to the test set. SHAP values take each data point into consideration when evaluating the importance of a feature. The below code snippet, takes the outcome of an xgBoost fitted model in xgb.mod and evaluates its SHAP values.

```{r}
# Convert test set to a matrix
test_matrix <- as.matrix(test[4:12])

shap_values=shap.values(xgb_model = xgb.mod, 
                        X_train = test_matrix)

# These scores quantify the contribution of each feature to the model's predictions for a specific instance. 
#Higher absolute SHAP values indicate greater importance of the feature in influencing the prediction.

shap_values$mean_shap_score[1:9]
```

the order of importance for those features which re deemed as importance by both methods is not consistent. The code snippet below limits the shap score and test set to the features selected by the Boruta method and plots SHAP values for each data point in test set diluted by a factor of 20.

```{r}
shap.feat=unique(boruta.imp.df$variable)
X=data.frame(as.matrix(test[4:12]))
X=X[, names(X) %in% shap.feat]

#shap summary for boruta features
shap_score_filtered=data.frame(shap_values$shap_score)

#plot
shap.plot.summary.wrap2(shap_score = shap_score_filtered, 
                        X = X)
                        #dilute=)
```


## Final Model 

So this would be based on the iterations and tuning that was explored. 

```{r}

```

# {-}

# Model prediction and performance {.tabset}

```{r}
# predict(xgb.train$finalModel, newdata = test)[1:5]
# 
# predict(svmFit, newdata = testDescr)[1:5]
# models <- list(svm = svmFit,
#                gbm = gbmFit)
# testPred <- predict(models, newdata = testDescr)
# lapply(testPred,
#        function(x) x[1:5])
# predValues <- extractPrediction(
#                                 models,
#                                 testX = testDescr,
#                                 testY = testClass)
# 
# testValues <- subset(
#                      predValues,
#                      dataType == "Test")
# head(testValues)
# table(testValues$model)
# nrow(testDescr)
# 
# probValues <- extractProb(
#                           models,
#                           testX = testDescr,
#                           testY = testClass)
# 
# testProbs <- subset(
#                     probValues,
#                     dataType == "Test")
# str(testProbs)
```

# {-}
